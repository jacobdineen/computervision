{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-__RAO5t53G"
   },
   "source": [
    "## Deep Learning Basics\n",
    "\n",
    "In this tutorial we we will implement a toy deep learning framework, to rapidly create neural network models. It should have some flexibilty and will be implemented so that new modules can be added and replaced just like in modern deep learning frameworks such as pytorch. We will be implementing some types of basic layers. We will also explore pytorch's nn package and see how pytorch accomplishes this goal and what are the extras that pytorch offers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "Y5AUSc3jufKi"
   },
   "source": [
    "### 1. Cleaner mini-batch-capable Linear + Softmax classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Vby-ogCCxYwa"
   },
   "source": [
    "In the previous assignment, we provided a basic implementation of a linear softmax classifier. In this assignment we will use this as a starting point for our deep learning library. \n",
    "\n",
    "**Softmax + Negative Log-Likelihood:** First we will re-implement the (softmax + negative log likelihood loss) computation, and its gradient computation. But we will additionally support batches of inputs and labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T02:54:51.021244Z",
     "start_time": "2020-02-11T02:54:51.006284Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "hidden": true,
    "id": "j0JJRfD6ti7m",
    "outputId": "f074b640-7e14-4880-ea95-83493870e422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input predictions:  torch.Size([32, 10])\n",
      "Input labels:  torch.Size([32, 1])\n",
      "Output loss:  2.295161008834839\n",
      "Input prediction gradients:  torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# This class combines Softmax + Negative-log likelihood loss.\n",
    "# Similar to the previous lab, but this implementation works for\n",
    "# batches of inputs and not just individual input vectors.\n",
    "# Here \"inputs\" is batchSize x sizePredictionScores, and\n",
    "#      \"labels\" is a vector of size batchSize.\n",
    "class toynn_CrossEntropyLoss(object):\n",
    "\n",
    "    # Forward pass: -log softmax(input_{label})\n",
    "    def forward(self, scores, labels):\n",
    "\n",
    "        # 1. Computing the softmax: exp(x) / sum (exp(x))\n",
    "        max_val = scores.max()  # This is to avoid variable overflows.\n",
    "        exp_inputs = (scores - max_val).exp()\n",
    "        # This is different than in the previous lab. Avoiding for loops here.\n",
    "        denominators = exp_inputs.sum(1).repeat(scores.size(1), 1).t()\n",
    "        self.predictions = torch.mul(exp_inputs, 1 / denominators)\n",
    "\n",
    "        # 2. Computing the loss: -log(y_label).\n",
    "        # Check what gather does. Just avoiding another for loop here.\n",
    "        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()\n",
    "\n",
    "    # Backward pass: y_hat - y\n",
    "    def backward(self, scores, labels):\n",
    "\n",
    "        # Here we avoid computing softmax again in backward pass.\n",
    "        grad_inputs = self.predictions.clone()\n",
    "\n",
    "        # Ok, Here we will use a for loop (but it is avoidable too).\n",
    "        for i in range(0, scores.size(0)):\n",
    "            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1\n",
    "\n",
    "        return grad_inputs\n",
    "\n",
    "\n",
    "# Let's verify if the above seems to be Okay.\n",
    "batchSize = 32\n",
    "mock_scores = torch.zeros(batchSize, 10).normal_(0, 0.1)\n",
    "mock_labels = torch.zeros(batchSize, 1, dtype=torch.long).fill_(3)\n",
    "\n",
    "loss_fn = toynn_CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn.forward(mock_scores, mock_labels)\n",
    "mock_scores_grads = loss_fn.backward(mock_scores, mock_labels)\n",
    "\n",
    "print(\"Input predictions: \", mock_scores.shape)\n",
    "print(\"Input labels: \", mock_labels.shape)\n",
    "print(\"Output loss: \", loss.item())\n",
    "print(\"Input prediction gradients: \", mock_scores_grads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "-DfaJM7RWk_0"
   },
   "source": [
    "\n",
    "**Linear Transformation:**  Next, we will re-implement the linear transformation computation $y=Wx+b$, and its gradient computation. But we will additionally support batches of inputs and labels. Making a batched implementation of this layer is easier because the only change is that now we have matrix-matrix multiplications as opposed to vector-matrix multiplications. Additionally, we will support returning the gradients with respect to the inputs to the linear transformation ($\\partial \\ell / \\partial x_j$). Notice that in our previous assignment we were only concerned with computing $\\partial \\ell / \\partial w_{ij}$ and  $\\partial \\ell / \\partial b_i$ (gradients for the parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T04:16:40.202784Z",
     "start_time": "2020-02-09T04:16:40.182837Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "hidden": true,
    "id": "2Lfp8zLNXVo_",
    "outputId": "4a931a6a-8bc0-42d7-db5e-8a9236f48806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:  torch.Size([32, 784])\n",
      "Weights W:  torch.Size([784, 10])\n",
      "Biases b:  torch.Size([10])\n",
      "Outputs:  torch.Size([32, 10])\n",
      "dL / dx:  torch.Size([32, 784])\n"
     ]
    }
   ],
   "source": [
    "class toynn_Linear(object):\n",
    "    def __init__(self, numInputs, numOutputs):\n",
    "        # Allocate tensors for the weight and bias parameters.\n",
    "        self.weight = torch.Tensor(numInputs, numOutputs).normal_(0, 0.01)\n",
    "        self.weight_grads = torch.Tensor(numInputs, numOutputs)\n",
    "        self.bias = torch.Tensor(numOutputs).zero_()\n",
    "        self.bias_grads = torch.Tensor(numOutputs)\n",
    "    \n",
    "    # Forward pass, inputs is a matrix of size batchSize x numInputs.\n",
    "    # Notice that compared to the previous assignment, each input vector\n",
    "    # is a row in this matrix.\n",
    "    def forward(self, inputs):\n",
    "        # This one needs no change, it just becomes \n",
    "        # a matrix x matrix multiplication\n",
    "        # as opposed to just vector x matrix multiplication as we had before.\n",
    "        return torch.matmul(inputs, self.weight) + self.bias\n",
    "    \n",
    "    # Backward pass, in addition to compute gradients for the weight and bias.\n",
    "    # It has to compute gradients with respect to inputs. \n",
    "    def backward(self, inputs, scores_grads):\n",
    "        self.weight_grads = torch.matmul(inputs.t(), scores_grads)\n",
    "        self.bias_grads = scores_grads.sum(0)\n",
    "        return torch.matmul(scores_grads, self.weight.t())\n",
    "\n",
    "# Input: batchSize x numInputs.\n",
    "numInputs = 1 * 28 * 28\n",
    "mock_inputs = torch.Tensor(batchSize, numInputs).normal_(0, 0.1)\n",
    "\n",
    "# Create the linear object to use.\n",
    "linear = toynn_Linear(numInputs, 10)\n",
    "\n",
    "# Forward and Backward passes:\n",
    "scores = linear.forward(mock_inputs)\n",
    "mock_inputs_grads = linear.backward(mock_inputs, mock_scores_grads)\n",
    "\n",
    "print(\"Input x: \", mock_inputs.shape)\n",
    "print(\"Weights W: \", linear.weight.shape)\n",
    "print(\"Biases b: \", linear.bias.shape)\n",
    "print(\"Outputs: \", scores.shape)\n",
    "print(\"dL / dx: \", mock_inputs_grads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "NNiPMoctZ5T_"
   },
   "source": [
    "We are finished with a cleaner implementation of the linear + softmax + negative log-likelihood classifier that we implemented for the previous assignment: (1) It supports batches,  (2) the functions for forward and backward are nicely packaged in a python class, (3) the weight and bias matrices (as well as weight_grad and bias_grad matrices) are nicely created and initialized in the constructor of the toynn_Linear class, (4) the Linear class also computes $\\partial \\ell / \\partial x_j$'s, which will be useful to stack layers in order to train deeper models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "PGdZ-bmxb473"
   },
   "source": [
    "### 2. Mini-batch SGD on FashionMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "zUfYXo8McA14"
   },
   "source": [
    "Given the newly implemented toynn_CrossEntropyLoss and toynn_Linear classes, let's train a classifier which is exactly the same as in the previous assignment, but now we can use batches of examples, as opposed to single examples during training with Stochastic (mini-batch) Gradient Descent (SGD). There are a few changes to make to the code from the previous assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T04:17:24.334465Z",
     "start_time": "2020-02-09T04:16:40.205775Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "hidden": true,
    "id": "IJDmV7Xgb3RI",
    "outputId": "79bedca5-c761-4d29-d025-168a426a61be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00100, Avg-Loss: 1.6845, Accuracy: 0.5635\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 1.4278, Accuracy: 0.6233\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 1.2883, Accuracy: 0.6468\n",
      "Train-epoch 0. Iteration 00400, Avg-Loss: 1.1924, Accuracy: 0.6648\n",
      "Train-epoch 0. Iteration 00500, Avg-Loss: 1.1253, Accuracy: 0.6798\n",
      "Train-epoch 0. Iteration 00600, Avg-Loss: 1.0746, Accuracy: 0.6901\n",
      "Validation-epoch 0. Avg-Loss: 0.8133, Accuracy: 0.7363\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 0.7722, Accuracy: 0.7651\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 0.7669, Accuracy: 0.7645\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 0.7598, Accuracy: 0.7658\n",
      "Train-epoch 1. Iteration 00400, Avg-Loss: 0.7511, Accuracy: 0.7680\n",
      "Train-epoch 1. Iteration 00500, Avg-Loss: 0.7389, Accuracy: 0.7707\n",
      "Train-epoch 1. Iteration 00600, Avg-Loss: 0.7322, Accuracy: 0.7722\n",
      "Validation-epoch 1. Avg-Loss: 0.7020, Accuracy: 0.7713\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 0.6760, Accuracy: 0.7952\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 0.6761, Accuracy: 0.7907\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 0.6701, Accuracy: 0.7911\n",
      "Train-epoch 2. Iteration 00400, Avg-Loss: 0.6652, Accuracy: 0.7926\n",
      "Train-epoch 2. Iteration 00500, Avg-Loss: 0.6603, Accuracy: 0.7933\n",
      "Train-epoch 2. Iteration 00600, Avg-Loss: 0.6559, Accuracy: 0.7948\n",
      "Validation-epoch 2. Avg-Loss: 0.6512, Accuracy: 0.7874\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 0.6396, Accuracy: 0.8086\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 0.6258, Accuracy: 0.8071\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 0.6255, Accuracy: 0.8045\n",
      "Train-epoch 3. Iteration 00400, Avg-Loss: 0.6211, Accuracy: 0.8052\n",
      "Train-epoch 3. Iteration 00500, Avg-Loss: 0.6157, Accuracy: 0.8057\n",
      "Train-epoch 3. Iteration 00600, Avg-Loss: 0.6141, Accuracy: 0.8049\n",
      "Validation-epoch 3. Avg-Loss: 0.6191, Accuracy: 0.7959\n",
      "Train-epoch 4. Iteration 00100, Avg-Loss: 0.5936, Accuracy: 0.8167\n",
      "Train-epoch 4. Iteration 00200, Avg-Loss: 0.5889, Accuracy: 0.8148\n",
      "Train-epoch 4. Iteration 00300, Avg-Loss: 0.5925, Accuracy: 0.8127\n",
      "Train-epoch 4. Iteration 00400, Avg-Loss: 0.5932, Accuracy: 0.8126\n",
      "Train-epoch 4. Iteration 00500, Avg-Loss: 0.5891, Accuracy: 0.8134\n",
      "Train-epoch 4. Iteration 00600, Avg-Loss: 0.5870, Accuracy: 0.8133\n",
      "Validation-epoch 4. Avg-Loss: 0.5981, Accuracy: 0.8022\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# Removes, the need to call F.to_image ourselves.\n",
    "# Also, please look up what transforms.Normalize does.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training, and validation datasets.\n",
    "trainset = FashionMNIST(root = './data', train = True, transform = transform, download = True)\n",
    "valset = FashionMNIST(root = './data', train = False, transform = transform, download = True)\n",
    "\n",
    "# NEW: Pytorch DataLoader for iterating over batches.\n",
    "batchSize = 100\n",
    "\n",
    "# Shuffling is needed in case dataset is not shuffled by default.\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                           batch_size = batchSize,\n",
    "                                           shuffle = True)\n",
    "# We don't need to bach the validation set but let's do it anyway.\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                         batch_size = batchSize,\n",
    "                                         shuffle = False) # No need.\n",
    "\n",
    "# Define a learning rate. \n",
    "learningRate = 1e-4\n",
    "\n",
    "# Define number of epochs.\n",
    "N = 5\n",
    "\n",
    "# Create the model.\n",
    "loss_fn = toynn_CrossEntropyLoss()\n",
    "linear_fn = toynn_Linear(1 * 28 * 28, 10)\n",
    "\n",
    "\n",
    "# log accuracies and losses.\n",
    "train_accuracies = []; val_accuracies = []\n",
    "train_losses = []; val_losses = []\n",
    "\n",
    "# Training loop. Please make sure you understand every single line of code below.\n",
    "# Go back to some of the previous steps in this lab if necessary.\n",
    "for epoch in range(0, N):\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    \n",
    "    # Make a pass over the training data.\n",
    "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
    "        \n",
    "        # Forward pass. (Prediction stage)\n",
    "        scores = linear_fn.forward(inputs)\n",
    "        cum_loss += loss_fn.forward(scores, labels).item()\n",
    "        \n",
    "        # Count how many correct in this batch.\n",
    "        max_scores, max_labels = scores.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "        \n",
    "        #Backward pass. (Gradient computation stage)\n",
    "        scores_grads = loss_fn.backward(scores, labels)\n",
    "        grad_inputs = linear_fn.backward(inputs, scores_grads)\n",
    "        \n",
    "        # Parameter updates (SGD step).\n",
    "        linear_fn.weight.add_(-learningRate, linear_fn.weight_grads)\n",
    "        linear_fn.bias.add_(-learningRate, linear_fn.bias_grads)\n",
    "        \n",
    "        # Logging the current results on training.\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i * batchSize + 1)))\n",
    "    \n",
    "    train_accuracies.append(correct / len(trainset))\n",
    "    train_losses.append(cum_loss / len(trainset))\n",
    "    \n",
    "    \n",
    "    # Make a pass over the validation data.\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "        inputs = inputs.view(batchSize, 1 * 28 * 28)\n",
    "        \n",
    "        # Forward pass. (Prediction stage)\n",
    "        scores = linear_fn.forward(inputs)\n",
    "        cum_loss += loss_fn.forward(scores, labels).item()\n",
    "        \n",
    "         # Count how many correct in this batch.\n",
    "        max_scores, max_labels = scores.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "          \n",
    "    val_accuracies.append(correct / len(valset))\n",
    "    val_losses.append(cum_loss / (i + 1))\n",
    "            \n",
    "    # Logging the current results on validation.\n",
    "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "          (epoch, cum_loss / (i + 1), correct / len(valset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "NheOKVEvktSU"
   },
   "source": [
    "We achieved an accuracy of 80% a lot faster than in the previous assignment, in fact we reached almost 83% in the same amount of epochs as before, so it seems mini-batching is helping to some extent. Try experimenting with different batch sizes and learning rates. Batch-size and learning rate are two hyper-parameters that are related in the optimization process, and is a current line of active research. For instance, under deep learning models, larger batch sizes do not offer as good generalization as smaller batches, which is bad because it is easier to parallelize code when using larger batch sizes. Also, a common strategy during training is to reduce (decay) the learning rate, as the training progresses to the later epochs, but a recent paper proposed to increase the batch size instead (for more, see: https://arxiv.org/abs/1711.00489 ). Probably for small datasets, keeping the learning rate fixed and batch size fixed will be fine, but as one moves to larger datasets, and deeper models, it becomes crucial to use some more advanced strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "rL8RK38atNE5"
   },
   "source": [
    "### 3. The Rectified Linear Unit (ReLU) Activation Function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "w2SukZfmtbH_"
   },
   "source": [
    "We are close to implementing a neural network, we can accomplish this by stacking a linear operation on top of a Rectified Linear Unit (ReLU) activation, another linear operation, and the softmax + negative log likelihood loss. This is all it takes to create simple neural network with one hidden layer. First, let's implement the ReLU layer as we implemented the linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T02:55:04.723643Z",
     "start_time": "2020-02-11T02:55:04.711676Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "hidden": true,
    "id": "i6ZplFBMwAkD",
    "outputId": "27583c1d-0344-4dc5-884d-3de420693c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:  tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
      "Output:  tensor([0.0000, 2.3000, 3.1000, 0.0000, 4.3000])\n",
      "Grad x:  tensor([0., 1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "class toynn_ReLU(object):\n",
    "  \n",
    "    # Forward operation: f(x_i) = max(0, x_i)\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs.clone()\n",
    "        outputs[outputs < 0] = 0\n",
    "        return outputs\n",
    "    \n",
    "    # Make sure the backward pass is absolutely clear.\n",
    "    def backward(self, inputs, outputs_grad):\n",
    "        inputs_grad = outputs_grad.clone() # 1 * previous_grads\n",
    "        inputs_grad[inputs < 0] = 0  # or zero.\n",
    "        return inputs_grad\n",
    "      \n",
    "# Let's test it.\n",
    "x = torch.tensor([-2.3, 2.3, 3.1, -1.3, 4.3])\n",
    "relu_fn = toynn_ReLU()\n",
    "\n",
    "print(\"Input x: \", x)\n",
    "print(\"Output: \", relu_fn.forward(x))\n",
    "print(\"Grad x: \", relu_fn.backward(x, torch.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "ajJ1z9qb0oqt"
   },
   "source": [
    "### 4. Forward Pass in a Two-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "LK4M717G0v-8"
   },
   "source": [
    "We are going to show here how to perform inference in a two-layer neural network using the operations defined earlier. In the questions for the assignment it is your task to train this network and demonstrate superior accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T04:17:24.625196Z",
     "start_time": "2020-02-09T04:17:24.354412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "hidden": true,
    "id": "pH4e6WI21DHw",
    "outputId": "f3fc60cb-8053-4ba7-b113-dcff9121d726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1012, 0.0980, 0.0986, 0.0984, 0.1002, 0.1020, 0.0997, 0.0950, 0.1026,\n",
      "         0.1043]])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('image', cmap = 'gray')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup the input variable x.\n",
    "img, label = trainset[0]\n",
    "x = img.view(1, 1 * 28 * 28)\n",
    "\n",
    "# Setup the number of inputs, hidden neurons, and outputs.\n",
    "nInputs = 1 * 28 * 28\n",
    "nHidden = 512\n",
    "nOutputs = 10\n",
    "\n",
    "# Create the model here.\n",
    "linear_fn1 = toynn_Linear(nInputs, nHidden)\n",
    "relu_fn = toynn_ReLU()\n",
    "linear_fn2 = toynn_Linear(nHidden, nOutputs)\n",
    "\n",
    "# Make predictions.\n",
    "a = linear_fn1.forward(x)\n",
    "z = relu_fn.forward(a)\n",
    "yhat = linear_fn2.forward(z)\n",
    "\n",
    "# Show the prediction scores for each class.\n",
    "# Yes, pytorch tensors already come with a softmax function.\n",
    "# We need it here because we hard-coded the softmax inside \n",
    "# the loss function.\n",
    "print(yhat.softmax(dim = 1)) \n",
    "\n",
    "plt.imshow(img[0]); plt.axis('off'); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "8I1rONJT53Wu"
   },
   "source": [
    "Since the weights, and biases in the two linear layers [linear_fn1, linear_fn2] are not trained, the predictions are arbitrary at this point. One of the tasks for this assignment is for you to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_8eQuKBi7u0"
   },
   "source": [
    "## Assignment Questions [100pts + 30pts (optional extra credit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "HpPb-CRh47Ct"
   },
   "source": [
    "### 1. Activation Functions (30pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "gJrvZxV6oDPj"
   },
   "source": [
    "Provide code for the following activation functions:\n",
    "\n",
    "$$\\text{Sigmoid(x)} = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}$$\n",
    "\n",
    "$$\\text{Tanh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "$$ \\text{LeakyReLU}(x) = \\begin{cases} \n",
    "      0.01 x & x < 0 \\\\\n",
    "      x & x \\geq 0 \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T02:55:24.659377Z",
     "start_time": "2020-02-11T02:55:24.622501Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "hidden": true,
    "id": "0ReR6YZC47eW",
    "outputId": "b9942797-8abd-452f-95a6-e5eaee369a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Sigmoid Example\n",
      "--------------------------------------------------\n",
      "input: tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
      "output: tensor([0.0911, 0.9089, 0.9569, 0.2142, 0.9866])\n",
      "grad tensor([0.0828, 0.0828, 0.0412, 0.1683, 0.0132])\n",
      "--------------------------------------------------\n",
      "Tanh Example\n",
      "--------------------------------------------------\n",
      "input: tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
      "output: tensor([-0.9801,  0.9801,  0.9959, -0.8617,  0.9996])\n",
      "grad tensor([0.0394, 0.0394, 0.0081, 0.2574, 0.0007])\n",
      "--------------------------------------------------\n",
      "Leaky Relu Example\n",
      "--------------------------------------------------\n",
      "input: tensor([-2.3000,  2.3000,  3.1000, -1.3000,  4.3000])\n",
      "output: tensor([-0.0230,  2.3000,  3.1000, -0.0130,  4.3000])\n",
      "grad tensor([0.0100, 1.0000, 1.0000, 0.0100, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid of x.\n",
    "import numpy as np\n",
    "import torch\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "class toynn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "        outputs = x.clone()\n",
    "        outputs = x.exp() / (x.exp() + 1)\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, x, output_grads):\n",
    "        # Backward pass\n",
    "        inputs_grad = output_grads.clone() \n",
    "        sigmoid = x.exp() / (x.exp() + 1)\n",
    "        return np.multiply(np.multiply(sigmoid, (1 - sigmoid)), inputs_grad)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# Hyperbolic tangent.\n",
    "class toynn_Tanh:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "        outputs = x.clone()\n",
    "        outputs = np.tanh(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, x, output_grads):\n",
    "        # Backward pass\n",
    "        inputs_grad = output_grads.clone() \n",
    "        tanh = np.tanh(x)\n",
    "        return np.multiply(1 - (tanh ** 2), inputs_grad)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# LeakyReLU of x.\n",
    "class toynn_LeakyReLU:\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs.clone()\n",
    "        outputs[outputs < 0] *= .01\n",
    "        return outputs\n",
    "    \n",
    "    # Make sure the backward pass is absolutely clear.\n",
    "    def backward(self, inputs, outputs_grad):\n",
    "        inputs_grad = outputs_grad.clone() \n",
    "        inputs_grad[inputs < 0] = .01  \n",
    "        inputs_grad[inputs >= 0] = 1  \n",
    "        return np.multiply(inputs_grad, outputs_grad)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('-' * 50)\n",
    "print('Sigmoid Example')\n",
    "print('-' * 50)\n",
    "x = torch.tensor([-2.3, 2.3, 3.1, -1.3, 4.3])\n",
    "sigmoid = toynn_Sigmoid()\n",
    "print('input:', x)\n",
    "print('output:', sigmoid.forward(x))\n",
    "print('grad', sigmoid.backward(x, torch.ones(5)))\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('-' * 50)\n",
    "print('Tanh Example')\n",
    "print('-' * 50)\n",
    "tanh = toynn_Tanh()\n",
    "print('input:', x)\n",
    "print('output:', tanh.forward(x))\n",
    "print('grad', tanh.backward(x, torch.ones(5)))\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('-' * 50)\n",
    "print('Leaky Relu Example')\n",
    "print('-' * 50)\n",
    "leaky_relu = toynn_LeakyReLU()\n",
    "print('input:', x)\n",
    "print('output:', leaky_relu.forward(x))\n",
    "print('grad', leaky_relu.backward(x, torch.ones(5)))\n",
    "#-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAMnCGwv-zJN"
   },
   "source": [
    "### 2. Binary Cross Entropy (BCE) loss function: (20pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0BccWLcn7Q1"
   },
   "source": [
    "Provide code for the binary cross entropy loss function as defined below:\n",
    "\n",
    "$$\\ell(y, \\hat{y}) = -\\sum_{i=1}^{i=n} [y_i\\text{log}(\\hat{y}_i) + (1 - y_i)\\text{log}(1 - \\hat{y}_i)]$$,\n",
    "\n",
    "where $n$ is the number of outputs (e.g. the size of vectors $y$ and $\\hat{y}$), the entries in the target vector $y_i$ are binary $\\in \\{0,1\\}$ and $y_i$ are typically the outputs of a sigmoid layer. Remember that the backward pass does not return a scalar but a vector containing the values for $\\partial \\ell / \\partial \\hat{y}_i$, henceforth a vector of the same size as $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T02:55:52.809164Z",
     "start_time": "2020-02-11T02:55:52.782239Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "zVC2BJpa-5Ka",
    "outputId": "bc982a29-7b2c-4fdd-a997-a45b53976772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Raw Numpy / Torch\n",
      "--------------------------------------------------\n",
      "x: tensor([0.4018, 0.2160, 0.3881])\n",
      "y: tensor([0., 0., 0.])\n",
      "loss: tensor(0.4161)\n",
      "grads tensor([1.6716, 1.2755, 1.6343])\n",
      "--------------------------------------------------\n",
      "Torch\n",
      "--------------------------------------------------\n",
      "x: tensor([0.4018, 0.2160, 0.3881])\n",
      "y: tensor([0., 0., 0.])\n",
      "loss: tensor(0.4161)\n",
      "[tensor(1.6716), tensor(1.2755), tensor(1.6343)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Binary cross entropy loss.\n",
    "# Useful for classification when the classes are not mutually exclusive.\n",
    "# For instance, when both shoe, and dress, are correct labels for an image.\n",
    "# In other words, when images have multiple labels per image.\n",
    "class toynn_BCELoss:\n",
    "    def forward(self, predictions, targets):\n",
    "        # Forward pass.\n",
    "\n",
    "        return -((predictions.log() * targets) +\n",
    "                 ((1 - targets) * (1 - predictions).log())).mean()\n",
    "\n",
    "    def backward(self, predictions, targets):\n",
    "        # Backward pass.\n",
    "        grad_inputs = predictions.clone()\n",
    "        lst = []\n",
    "        for i in range(len(predictions)):\n",
    "            #grad_inputs[i] = -np.log(predictions[i]) * targets[i] + -np.log(1 - predictions[i]) * (1 - targets[i])\n",
    "            grad_inputs[i] = (targets[i] / predictions[i]) + ((1-targets[i]) / (1- predictions[i]))\n",
    "        return grad_inputs\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "x0 = torch.randn(3)\n",
    "y = torch.FloatTensor(3).random_(2)\n",
    "\n",
    "\n",
    "print('-' * 50)\n",
    "print('Raw Numpy / Torch')\n",
    "print('-' * 50)\n",
    "bse_loss = toynn_BCELoss()\n",
    "\n",
    "\n",
    "x = sigmoid.forward(x0)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "loss = bse_loss.forward(predictions=x, targets=y)\n",
    "print('loss:', loss)\n",
    "print('grads',bse_loss.backward(predictions=x, targets=y))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('-' * 50)\n",
    "print('Torch') #https://github.com/yang-zhang/yang-zhang.github.io/blob/master/ds_code/pytorch-losses-in-plain-python.ipynb\n",
    "print('-' * 50)\n",
    "loss_fcn = nn.BCELoss()\n",
    "x = nn.Sigmoid()(x0)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('loss:', loss_fcn(x, y))\n",
    "\n",
    "grads = []\n",
    "for i in range(len(x)):\n",
    "    #grad_inputs[i] = -np.log(predictions[i]) * targets[i] + -np.log(1 - predictions[i]) * (1 - targets[i])\n",
    "    grads.append((y[i] / x[i]) + ((1-y[i]) / (1- x[i])))\n",
    "print(grads)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGIxg3q5i_bl"
   },
   "source": [
    "### 3. Training of the Two Layer NN (50pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5SiFBConq67"
   },
   "source": [
    "Train the two-layer Neural Network as defined in Section 3 of this Assignment on FashionMNIST using toynn. Include below the code for training this neural network and report the training, validation plots for loss and accuracy. The code should be similar to the code in Section 2 of this assignment, please follow that convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T04:26:58.586097Z",
     "start_time": "2020-02-09T04:17:24.731943Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UzQc9ZtKjnf0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00150, Avg-Loss: 2.0956, Accuracy: 0.3921\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 1.4772, Accuracy: 0.5370\n",
      "Validation-epoch 0. Avg-Loss: 0.7552, Accuracy: 0.7139\n",
      "Train-epoch 1. Iteration 00150, Avg-Loss: 0.6913, Accuracy: 0.7412\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 0.6609, Accuracy: 0.7518\n",
      "Validation-epoch 1. Avg-Loss: 0.6029, Accuracy: 0.7741\n",
      "Train-epoch 2. Iteration 00150, Avg-Loss: 0.5850, Accuracy: 0.7860\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 0.5700, Accuracy: 0.7922\n",
      "Validation-epoch 2. Avg-Loss: 0.5850, Accuracy: 0.7907\n",
      "Train-epoch 3. Iteration 00150, Avg-Loss: 0.5304, Accuracy: 0.8152\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 0.5204, Accuracy: 0.8138\n",
      "Validation-epoch 3. Avg-Loss: 0.5319, Accuracy: 0.8040\n",
      "Train-epoch 4. Iteration 00150, Avg-Loss: 0.4929, Accuracy: 0.8284\n",
      "Train-epoch 4. Iteration 00300, Avg-Loss: 0.4938, Accuracy: 0.8244\n",
      "Validation-epoch 4. Avg-Loss: 0.5101, Accuracy: 0.8122\n",
      "Train-epoch 5. Iteration 00150, Avg-Loss: 0.4772, Accuracy: 0.8350\n",
      "Train-epoch 5. Iteration 00300, Avg-Loss: 0.4731, Accuracy: 0.8331\n",
      "Validation-epoch 5. Avg-Loss: 0.4905, Accuracy: 0.8191\n",
      "Train-epoch 6. Iteration 00150, Avg-Loss: 0.4577, Accuracy: 0.8434\n",
      "Train-epoch 6. Iteration 00300, Avg-Loss: 0.4604, Accuracy: 0.8380\n",
      "Validation-epoch 6. Avg-Loss: 0.4791, Accuracy: 0.8271\n",
      "Train-epoch 7. Iteration 00150, Avg-Loss: 0.4503, Accuracy: 0.8439\n",
      "Train-epoch 7. Iteration 00300, Avg-Loss: 0.4469, Accuracy: 0.8426\n",
      "Validation-epoch 7. Avg-Loss: 0.4704, Accuracy: 0.8270\n",
      "Train-epoch 8. Iteration 00150, Avg-Loss: 0.4415, Accuracy: 0.8475\n",
      "Train-epoch 8. Iteration 00300, Avg-Loss: 0.4390, Accuracy: 0.8464\n",
      "Validation-epoch 8. Avg-Loss: 0.4620, Accuracy: 0.8346\n",
      "Train-epoch 9. Iteration 00150, Avg-Loss: 0.4281, Accuracy: 0.8539\n",
      "Train-epoch 9. Iteration 00300, Avg-Loss: 0.4292, Accuracy: 0.8510\n",
      "Validation-epoch 9. Avg-Loss: 0.4580, Accuracy: 0.8364\n",
      "Train-epoch 10. Iteration 00150, Avg-Loss: 0.4258, Accuracy: 0.8547\n",
      "Train-epoch 10. Iteration 00300, Avg-Loss: 0.4244, Accuracy: 0.8514\n",
      "Validation-epoch 10. Avg-Loss: 0.4496, Accuracy: 0.8395\n",
      "Train-epoch 11. Iteration 00150, Avg-Loss: 0.4158, Accuracy: 0.8565\n",
      "Train-epoch 11. Iteration 00300, Avg-Loss: 0.4166, Accuracy: 0.8533\n",
      "Validation-epoch 11. Avg-Loss: 0.4637, Accuracy: 0.8286\n",
      "Train-epoch 12. Iteration 00150, Avg-Loss: 0.4121, Accuracy: 0.8595\n",
      "Train-epoch 12. Iteration 00300, Avg-Loss: 0.4101, Accuracy: 0.8563\n",
      "Validation-epoch 12. Avg-Loss: 0.4443, Accuracy: 0.8398\n",
      "Train-epoch 13. Iteration 00150, Avg-Loss: 0.4053, Accuracy: 0.8613\n",
      "Train-epoch 13. Iteration 00300, Avg-Loss: 0.4070, Accuracy: 0.8578\n",
      "Validation-epoch 13. Avg-Loss: 0.4435, Accuracy: 0.8390\n",
      "Train-epoch 14. Iteration 00150, Avg-Loss: 0.4031, Accuracy: 0.8617\n",
      "Train-epoch 14. Iteration 00300, Avg-Loss: 0.4026, Accuracy: 0.8589\n",
      "Validation-epoch 14. Avg-Loss: 0.4356, Accuracy: 0.8440\n",
      "Train-epoch 15. Iteration 00150, Avg-Loss: 0.3957, Accuracy: 0.8648\n",
      "Train-epoch 15. Iteration 00300, Avg-Loss: 0.3966, Accuracy: 0.8613\n",
      "Validation-epoch 15. Avg-Loss: 0.4323, Accuracy: 0.8449\n",
      "Train-epoch 16. Iteration 00150, Avg-Loss: 0.3980, Accuracy: 0.8630\n",
      "Train-epoch 16. Iteration 00300, Avg-Loss: 0.3921, Accuracy: 0.8635\n",
      "Validation-epoch 16. Avg-Loss: 0.4267, Accuracy: 0.8467\n",
      "Train-epoch 17. Iteration 00150, Avg-Loss: 0.3920, Accuracy: 0.8653\n",
      "Train-epoch 17. Iteration 00300, Avg-Loss: 0.3892, Accuracy: 0.8630\n",
      "Validation-epoch 17. Avg-Loss: 0.4241, Accuracy: 0.8472\n",
      "Train-epoch 18. Iteration 00150, Avg-Loss: 0.3854, Accuracy: 0.8667\n",
      "Train-epoch 18. Iteration 00300, Avg-Loss: 0.3846, Accuracy: 0.8657\n",
      "Validation-epoch 18. Avg-Loss: 0.4357, Accuracy: 0.8459\n",
      "Train-epoch 19. Iteration 00150, Avg-Loss: 0.3808, Accuracy: 0.8702\n",
      "Train-epoch 19. Iteration 00300, Avg-Loss: 0.3810, Accuracy: 0.8667\n",
      "Validation-epoch 19. Avg-Loss: 0.4340, Accuracy: 0.8406\n",
      "Train-epoch 20. Iteration 00150, Avg-Loss: 0.3824, Accuracy: 0.8689\n",
      "Train-epoch 20. Iteration 00300, Avg-Loss: 0.3780, Accuracy: 0.8681\n",
      "Validation-epoch 20. Avg-Loss: 0.4171, Accuracy: 0.8493\n",
      "Train-epoch 21. Iteration 00150, Avg-Loss: 0.3748, Accuracy: 0.8720\n",
      "Train-epoch 21. Iteration 00300, Avg-Loss: 0.3740, Accuracy: 0.8697\n",
      "Validation-epoch 21. Avg-Loss: 0.4415, Accuracy: 0.8429\n",
      "Train-epoch 22. Iteration 00150, Avg-Loss: 0.3719, Accuracy: 0.8724\n",
      "Train-epoch 22. Iteration 00300, Avg-Loss: 0.3706, Accuracy: 0.8696\n",
      "Validation-epoch 22. Avg-Loss: 0.4075, Accuracy: 0.8552\n",
      "Train-epoch 23. Iteration 00150, Avg-Loss: 0.3686, Accuracy: 0.8747\n",
      "Train-epoch 23. Iteration 00300, Avg-Loss: 0.3675, Accuracy: 0.8723\n",
      "Validation-epoch 23. Avg-Loss: 0.4145, Accuracy: 0.8522\n",
      "Train-epoch 24. Iteration 00150, Avg-Loss: 0.3643, Accuracy: 0.8748\n",
      "Train-epoch 24. Iteration 00300, Avg-Loss: 0.3655, Accuracy: 0.8726\n",
      "Validation-epoch 24. Avg-Loss: 0.4088, Accuracy: 0.8550\n",
      "Train-epoch 25. Iteration 00150, Avg-Loss: 0.3637, Accuracy: 0.8721\n",
      "Train-epoch 25. Iteration 00300, Avg-Loss: 0.3628, Accuracy: 0.8719\n",
      "Validation-epoch 25. Avg-Loss: 0.4118, Accuracy: 0.8513\n",
      "Train-epoch 26. Iteration 00150, Avg-Loss: 0.3610, Accuracy: 0.8753\n",
      "Train-epoch 26. Iteration 00300, Avg-Loss: 0.3607, Accuracy: 0.8726\n",
      "Validation-epoch 26. Avg-Loss: 0.4022, Accuracy: 0.8560\n",
      "Train-epoch 27. Iteration 00150, Avg-Loss: 0.3552, Accuracy: 0.8790\n",
      "Train-epoch 27. Iteration 00300, Avg-Loss: 0.3564, Accuracy: 0.8750\n",
      "Validation-epoch 27. Avg-Loss: 0.4027, Accuracy: 0.8549\n",
      "Train-epoch 28. Iteration 00150, Avg-Loss: 0.3510, Accuracy: 0.8804\n",
      "Train-epoch 28. Iteration 00300, Avg-Loss: 0.3553, Accuracy: 0.8766\n",
      "Validation-epoch 28. Avg-Loss: 0.3932, Accuracy: 0.8580\n",
      "Train-epoch 29. Iteration 00150, Avg-Loss: 0.3515, Accuracy: 0.8789\n",
      "Train-epoch 29. Iteration 00300, Avg-Loss: 0.3533, Accuracy: 0.8761\n",
      "Validation-epoch 29. Avg-Loss: 0.3996, Accuracy: 0.8567\n",
      "Train-epoch 30. Iteration 00150, Avg-Loss: 0.3481, Accuracy: 0.8816\n",
      "Train-epoch 30. Iteration 00300, Avg-Loss: 0.3497, Accuracy: 0.8774\n",
      "Validation-epoch 30. Avg-Loss: 0.3958, Accuracy: 0.8560\n",
      "Train-epoch 31. Iteration 00150, Avg-Loss: 0.3507, Accuracy: 0.8803\n",
      "Train-epoch 31. Iteration 00300, Avg-Loss: 0.3460, Accuracy: 0.8783\n",
      "Validation-epoch 31. Avg-Loss: 0.3974, Accuracy: 0.8601\n",
      "Train-epoch 32. Iteration 00150, Avg-Loss: 0.3421, Accuracy: 0.8802\n",
      "Train-epoch 32. Iteration 00300, Avg-Loss: 0.3448, Accuracy: 0.8786\n",
      "Validation-epoch 32. Avg-Loss: 0.3980, Accuracy: 0.8558\n",
      "Train-epoch 33. Iteration 00150, Avg-Loss: 0.3439, Accuracy: 0.8813\n",
      "Train-epoch 33. Iteration 00300, Avg-Loss: 0.3424, Accuracy: 0.8804\n",
      "Validation-epoch 33. Avg-Loss: 0.3963, Accuracy: 0.8567\n",
      "Train-epoch 34. Iteration 00150, Avg-Loss: 0.3408, Accuracy: 0.8838\n",
      "Train-epoch 34. Iteration 00300, Avg-Loss: 0.3403, Accuracy: 0.8809\n",
      "Validation-epoch 34. Avg-Loss: 0.3866, Accuracy: 0.8629\n",
      "Train-epoch 35. Iteration 00150, Avg-Loss: 0.3382, Accuracy: 0.8843\n",
      "Train-epoch 35. Iteration 00300, Avg-Loss: 0.3374, Accuracy: 0.8823\n",
      "Validation-epoch 35. Avg-Loss: 0.3851, Accuracy: 0.8614\n",
      "Train-epoch 36. Iteration 00150, Avg-Loss: 0.3328, Accuracy: 0.8889\n",
      "Train-epoch 36. Iteration 00300, Avg-Loss: 0.3359, Accuracy: 0.8824\n",
      "Validation-epoch 36. Avg-Loss: 0.3942, Accuracy: 0.8588\n",
      "Train-epoch 37. Iteration 00150, Avg-Loss: 0.3329, Accuracy: 0.8841\n",
      "Train-epoch 37. Iteration 00300, Avg-Loss: 0.3339, Accuracy: 0.8821\n",
      "Validation-epoch 37. Avg-Loss: 0.3808, Accuracy: 0.8639\n",
      "Train-epoch 38. Iteration 00150, Avg-Loss: 0.3321, Accuracy: 0.8858\n",
      "Train-epoch 38. Iteration 00300, Avg-Loss: 0.3324, Accuracy: 0.8830\n",
      "Validation-epoch 38. Avg-Loss: 0.3828, Accuracy: 0.8634\n",
      "Train-epoch 39. Iteration 00150, Avg-Loss: 0.3250, Accuracy: 0.8884\n",
      "Train-epoch 39. Iteration 00300, Avg-Loss: 0.3300, Accuracy: 0.8843\n",
      "Validation-epoch 39. Avg-Loss: 0.3821, Accuracy: 0.8638\n",
      "Train-epoch 40. Iteration 00150, Avg-Loss: 0.3255, Accuracy: 0.8876\n",
      "Train-epoch 40. Iteration 00300, Avg-Loss: 0.3275, Accuracy: 0.8850\n",
      "Validation-epoch 40. Avg-Loss: 0.3828, Accuracy: 0.8623\n",
      "Train-epoch 41. Iteration 00150, Avg-Loss: 0.3260, Accuracy: 0.8874\n",
      "Train-epoch 41. Iteration 00300, Avg-Loss: 0.3268, Accuracy: 0.8855\n",
      "Validation-epoch 41. Avg-Loss: 0.3882, Accuracy: 0.8602\n",
      "Train-epoch 42. Iteration 00150, Avg-Loss: 0.3229, Accuracy: 0.8877\n",
      "Train-epoch 42. Iteration 00300, Avg-Loss: 0.3246, Accuracy: 0.8864\n",
      "Validation-epoch 42. Avg-Loss: 0.3833, Accuracy: 0.8610\n",
      "Train-epoch 43. Iteration 00150, Avg-Loss: 0.3276, Accuracy: 0.8884\n",
      "Train-epoch 43. Iteration 00300, Avg-Loss: 0.3217, Accuracy: 0.8875\n",
      "Validation-epoch 43. Avg-Loss: 0.3756, Accuracy: 0.8649\n",
      "Train-epoch 44. Iteration 00150, Avg-Loss: 0.3217, Accuracy: 0.8909\n",
      "Train-epoch 44. Iteration 00300, Avg-Loss: 0.3208, Accuracy: 0.8882\n",
      "Validation-epoch 44. Avg-Loss: 0.3849, Accuracy: 0.8615\n",
      "Train-epoch 45. Iteration 00150, Avg-Loss: 0.3155, Accuracy: 0.8926\n",
      "Train-epoch 45. Iteration 00300, Avg-Loss: 0.3182, Accuracy: 0.8887\n",
      "Validation-epoch 45. Avg-Loss: 0.3788, Accuracy: 0.8630\n",
      "Train-epoch 46. Iteration 00150, Avg-Loss: 0.3183, Accuracy: 0.8906\n",
      "Train-epoch 46. Iteration 00300, Avg-Loss: 0.3172, Accuracy: 0.8878\n",
      "Validation-epoch 46. Avg-Loss: 0.3716, Accuracy: 0.8692\n",
      "Train-epoch 47. Iteration 00150, Avg-Loss: 0.3175, Accuracy: 0.8915\n",
      "Train-epoch 47. Iteration 00300, Avg-Loss: 0.3153, Accuracy: 0.8894\n",
      "Validation-epoch 47. Avg-Loss: 0.3735, Accuracy: 0.8681\n",
      "Train-epoch 48. Iteration 00150, Avg-Loss: 0.3138, Accuracy: 0.8932\n",
      "Train-epoch 48. Iteration 00300, Avg-Loss: 0.3142, Accuracy: 0.8902\n",
      "Validation-epoch 48. Avg-Loss: 0.3752, Accuracy: 0.8665\n",
      "Train-epoch 49. Iteration 00150, Avg-Loss: 0.3077, Accuracy: 0.8960\n",
      "Train-epoch 49. Iteration 00300, Avg-Loss: 0.3117, Accuracy: 0.8914\n",
      "Validation-epoch 49. Avg-Loss: 0.3703, Accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# Removes, the need to call F.to_image ourselves.\n",
    "# Also, please look up what transforms.Normalize does.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training, and validation datasets.\n",
    "trainset = FashionMNIST(root = './data', train = True, transform = transform, download = True)\n",
    "valset = FashionMNIST(root = './data', train = False, transform = transform, download = True)\n",
    "\n",
    "# NEW: Pytorch DataLoader for iterating over batches.\n",
    "batchSize = 200\n",
    "\n",
    "# Shuffling is needed in case dataset is not shuffled by default.\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                           batch_size = batchSize,\n",
    "                                           shuffle = True)\n",
    "\n",
    "# We don't need to bach the validation set but let's do it anyway.\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                         batch_size = batchSize,\n",
    "                                         shuffle = False) # No need.\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# Define a learning rate.\n",
    "learningRate = 1e-3\n",
    "\n",
    "# Define number of epochs.\n",
    "N = 50\n",
    "\n",
    "# Create the model.\n",
    "n_inputs = 1 * 28 * 28\n",
    "nHidden = 1024  #Neurons\n",
    "nOutputs = 10  #10 classes on fashionmnist\n",
    "\n",
    "model = {}\n",
    "model['linear1'] = toynn_Linear(n_inputs, nHidden)\n",
    "model['sigmoid'] = toynn_Sigmoid()\n",
    "model['linear2'] = toynn_Linear(nHidden, nOutputs)\n",
    "model['loss'] = toynn_CrossEntropyLoss()\n",
    "\n",
    "# log accuracies and losses.\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training loop. Please make sure you understand every single line of code below.\n",
    "# Go back to some of the previous steps in this lab if necessary.\n",
    "for epoch in range(0, N):\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    # Make a pass over the training data.\n",
    "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "        # .view appears to be similar to np.reshape. We're just flattening and reshaping inputs into a new tensor shape here.\n",
    "        inputs = inputs.view(\n",
    "            batchSize,\n",
    "            n_inputs)  #Returns Torch Tensor batchSize X Image Size Flattened\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "        #Forward\n",
    "        a = model['linear1'].forward(inputs)  # torch.Size([100, 512])\n",
    "        z = model['sigmoid'].forward(a)  # torch.Size([100, 512])\n",
    "        y_hat = model['linear2'].forward(\n",
    "            z\n",
    "        )  # torch.Size([100, 10]) #This is computed as softmax in the loss for CrossEntropy\n",
    "\n",
    "        cum_loss += model['loss'].forward(y_hat, labels).item()\n",
    "\n",
    "        # Count how many correct in this batch.\n",
    "        max_scores, max_labels = y_hat.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        #Backward pass. (Gradient computation stage)\n",
    "        y_hat_grads = model['loss'].backward(y_hat,\n",
    "                                             labels)  #torch.Size([10, 10])\n",
    "        z_grads = model['linear2'].backward(z, y_hat_grads)\n",
    "        a_grads = model['sigmoid'].backward(a, z_grads)\n",
    "        x_grads = model['linear1'].backward(inputs, a_grads)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Parameter updates (SGD step).\n",
    "        model['linear1'].weight.add_(-learningRate,\n",
    "                                     model['linear1'].weight_grads)\n",
    "        model['linear1'].bias.add_(-learningRate, model['linear1'].bias_grads)\n",
    "        model['linear2'].weight.add_(-learningRate,\n",
    "                                     model['linear2'].weight_grads)\n",
    "        model['linear2'].bias.add_(-learningRate, model['linear2'].bias_grads)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Logging the current results on training.\n",
    "        if (i + 1) % 150 == 0:\n",
    "            print(\n",
    "                'Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f'\n",
    "                % (epoch, i + 1, cum_loss / (i + 1), correct /\n",
    "                   (i * batchSize + 1)))\n",
    "\n",
    "    train_accuracies.append(correct / len(trainset))\n",
    "    train_losses.append(cum_loss / len(trainset))\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Make a pass over the validation data.\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "        inputs = inputs.view(batchSize, n_inputs)\n",
    "\n",
    "        #Forward\n",
    "        a = model['linear1'].forward(inputs)  # torch.Size([100, 512])\n",
    "        z = model['sigmoid'].forward(a)  # torch.Size([100, 512])\n",
    "        #This is computed as softmax in the loss for CrossEntropy\n",
    "        y_hat = model['linear2'].forward(z)  # torch.Size([100, 10])\n",
    "\n",
    "        cum_loss += model['loss'].forward(y_hat, labels).item()\n",
    "\n",
    "        # Count how many correct in this batch.\n",
    "        max_scores, max_labels = y_hat.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "\n",
    "    val_accuracies.append(correct / len(valset))\n",
    "    val_losses.append(cum_loss / (i + 1))\n",
    "\n",
    "    # Logging the current results on validation.\n",
    "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "          (epoch, cum_loss / (i + 1), correct / len(valset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T04:51:46.139256Z",
     "start_time": "2020-02-09T04:51:45.910508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code for visualizing train/val plots for accuracy and losses.\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_losses, 'bo-', label='val-loss')\n",
    "plt.plot(train_losses, 'ro-', label='train-loss')\n",
    "plt.grid('on')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['validation', 'training'], loc='upper right')\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, 'bo-', label='val-acc')\n",
    "plt.plot(train_accuracies, 'ro-', label='train-acc')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid('on')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['validation', 'training'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXlKZ2eQj9Y7"
   },
   "source": [
    "### 4. The CIFAR 10 Dataset (torchvision.datasets.CIFAR10). (30pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZjEdGaYojsg"
   },
   "source": [
    "Include the code for defining, and training the neural network, as well as plots for training, validation for loss and accuracy. Show the predictions for one example of this dataset. Write down below your code in bold face the accuracy you obtained and in how many epochs. I will post the highest accuracies that people obtained for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:16:53.062799Z",
     "start_time": "2020-02-06T19:53:15.440150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train-epoch 0. Iteration 00100, Avg-Loss: 2.2928, Accuracy: 0.1250\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 2.2670, Accuracy: 0.1480\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 2.2239, Accuracy: 0.1697\n",
      "Train-epoch 0. Iteration 00400, Avg-Loss: 2.1848, Accuracy: 0.1891\n",
      "Train-epoch 0. Iteration 00500, Avg-Loss: 2.1511, Accuracy: 0.2055\n",
      "Validation-epoch 0. Avg-Loss: 1.9938, Accuracy: 26.9500\n",
      "--------------------------------------------------\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 1.9816, Accuracy: 0.2817\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 1.9634, Accuracy: 0.2942\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 1.9493, Accuracy: 0.2997\n",
      "Train-epoch 1. Iteration 00400, Avg-Loss: 1.9362, Accuracy: 0.3052\n",
      "Train-epoch 1. Iteration 00500, Avg-Loss: 1.9258, Accuracy: 0.3100\n",
      "Validation-epoch 1. Avg-Loss: 1.8672, Accuracy: 32.9200\n",
      "--------------------------------------------------\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 1.8735, Accuracy: 0.3349\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 1.8587, Accuracy: 0.3368\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 1.8544, Accuracy: 0.3369\n",
      "Train-epoch 2. Iteration 00400, Avg-Loss: 1.8503, Accuracy: 0.3393\n",
      "Train-epoch 2. Iteration 00500, Avg-Loss: 1.8428, Accuracy: 0.3421\n",
      "Validation-epoch 2. Avg-Loss: 1.7991, Accuracy: 36.1500\n",
      "--------------------------------------------------\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 1.7941, Accuracy: 0.3681\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 1.7930, Accuracy: 0.3675\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 1.7895, Accuracy: 0.3664\n",
      "Train-epoch 3. Iteration 00400, Avg-Loss: 1.7873, Accuracy: 0.3666\n",
      "Train-epoch 3. Iteration 00500, Avg-Loss: 1.7838, Accuracy: 0.3681\n",
      "Validation-epoch 3. Avg-Loss: 1.7512, Accuracy: 37.2700\n",
      "--------------------------------------------------\n",
      "Train-epoch 4. Iteration 00100, Avg-Loss: 1.7548, Accuracy: 0.3837\n",
      "Train-epoch 4. Iteration 00200, Avg-Loss: 1.7503, Accuracy: 0.3815\n",
      "Train-epoch 4. Iteration 00300, Avg-Loss: 1.7487, Accuracy: 0.3812\n",
      "Train-epoch 4. Iteration 00400, Avg-Loss: 1.7423, Accuracy: 0.3843\n",
      "Train-epoch 4. Iteration 00500, Avg-Loss: 1.7353, Accuracy: 0.3859\n",
      "Validation-epoch 4. Avg-Loss: 1.7007, Accuracy: 39.9000\n",
      "--------------------------------------------------\n",
      "Train-epoch 5. Iteration 00100, Avg-Loss: 1.6950, Accuracy: 0.3998\n",
      "Train-epoch 5. Iteration 00200, Avg-Loss: 1.7006, Accuracy: 0.3984\n",
      "Train-epoch 5. Iteration 00300, Avg-Loss: 1.6979, Accuracy: 0.3986\n",
      "Train-epoch 5. Iteration 00400, Avg-Loss: 1.6951, Accuracy: 0.4004\n",
      "Train-epoch 5. Iteration 00500, Avg-Loss: 1.6920, Accuracy: 0.4017\n",
      "Validation-epoch 5. Avg-Loss: 1.6631, Accuracy: 40.8300\n",
      "--------------------------------------------------\n",
      "Train-epoch 6. Iteration 00100, Avg-Loss: 1.6800, Accuracy: 0.4108\n",
      "Train-epoch 6. Iteration 00200, Avg-Loss: 1.6719, Accuracy: 0.4124\n",
      "Train-epoch 6. Iteration 00300, Avg-Loss: 1.6620, Accuracy: 0.4142\n",
      "Train-epoch 6. Iteration 00400, Avg-Loss: 1.6553, Accuracy: 0.4169\n",
      "Train-epoch 6. Iteration 00500, Avg-Loss: 1.6565, Accuracy: 0.4165\n",
      "Validation-epoch 6. Avg-Loss: 1.6251, Accuracy: 42.5100\n",
      "--------------------------------------------------\n",
      "Train-epoch 7. Iteration 00100, Avg-Loss: 1.6355, Accuracy: 0.4237\n",
      "Train-epoch 7. Iteration 00200, Avg-Loss: 1.6364, Accuracy: 0.4227\n",
      "Train-epoch 7. Iteration 00300, Avg-Loss: 1.6372, Accuracy: 0.4226\n",
      "Train-epoch 7. Iteration 00400, Avg-Loss: 1.6317, Accuracy: 0.4253\n",
      "Train-epoch 7. Iteration 00500, Avg-Loss: 1.6281, Accuracy: 0.4258\n",
      "Validation-epoch 7. Avg-Loss: 1.6081, Accuracy: 43.4000\n",
      "--------------------------------------------------\n",
      "Train-epoch 8. Iteration 00100, Avg-Loss: 1.6121, Accuracy: 0.4387\n",
      "Train-epoch 8. Iteration 00200, Avg-Loss: 1.6089, Accuracy: 0.4389\n",
      "Train-epoch 8. Iteration 00300, Avg-Loss: 1.6073, Accuracy: 0.4363\n",
      "Train-epoch 8. Iteration 00400, Avg-Loss: 1.5998, Accuracy: 0.4381\n",
      "Train-epoch 8. Iteration 00500, Avg-Loss: 1.6001, Accuracy: 0.4380\n",
      "Validation-epoch 8. Avg-Loss: 1.6121, Accuracy: 42.9600\n",
      "--------------------------------------------------\n",
      "Train-epoch 9. Iteration 00100, Avg-Loss: 1.5705, Accuracy: 0.4503\n",
      "Train-epoch 9. Iteration 00200, Avg-Loss: 1.5731, Accuracy: 0.4486\n",
      "Train-epoch 9. Iteration 00300, Avg-Loss: 1.5749, Accuracy: 0.4480\n",
      "Train-epoch 9. Iteration 00400, Avg-Loss: 1.5720, Accuracy: 0.4484\n",
      "Train-epoch 9. Iteration 00500, Avg-Loss: 1.5750, Accuracy: 0.4471\n",
      "Validation-epoch 9. Avg-Loss: 1.5733, Accuracy: 45.0700\n",
      "--------------------------------------------------\n",
      "Train-epoch 10. Iteration 00100, Avg-Loss: 1.5603, Accuracy: 0.4539\n",
      "Train-epoch 10. Iteration 00200, Avg-Loss: 1.5601, Accuracy: 0.4520\n",
      "Train-epoch 10. Iteration 00300, Avg-Loss: 1.5598, Accuracy: 0.4525\n",
      "Train-epoch 10. Iteration 00400, Avg-Loss: 1.5594, Accuracy: 0.4529\n",
      "Train-epoch 10. Iteration 00500, Avg-Loss: 1.5558, Accuracy: 0.4528\n",
      "Validation-epoch 10. Avg-Loss: 1.5538, Accuracy: 45.1500\n",
      "--------------------------------------------------\n",
      "Train-epoch 11. Iteration 00100, Avg-Loss: 1.5590, Accuracy: 0.4559\n",
      "Train-epoch 11. Iteration 00200, Avg-Loss: 1.5545, Accuracy: 0.4559\n",
      "Train-epoch 11. Iteration 00300, Avg-Loss: 1.5504, Accuracy: 0.4563\n",
      "Train-epoch 11. Iteration 00400, Avg-Loss: 1.5391, Accuracy: 0.4596\n",
      "Train-epoch 11. Iteration 00500, Avg-Loss: 1.5364, Accuracy: 0.4603\n",
      "Validation-epoch 11. Avg-Loss: 1.5328, Accuracy: 45.9900\n",
      "--------------------------------------------------\n",
      "Train-epoch 12. Iteration 00100, Avg-Loss: 1.5252, Accuracy: 0.4659\n",
      "Train-epoch 12. Iteration 00200, Avg-Loss: 1.5321, Accuracy: 0.4640\n",
      "Train-epoch 12. Iteration 00300, Avg-Loss: 1.5223, Accuracy: 0.4651\n",
      "Train-epoch 12. Iteration 00400, Avg-Loss: 1.5200, Accuracy: 0.4657\n",
      "Train-epoch 12. Iteration 00500, Avg-Loss: 1.5190, Accuracy: 0.4669\n",
      "Validation-epoch 12. Avg-Loss: 1.5134, Accuracy: 46.3200\n",
      "--------------------------------------------------\n",
      "Train-epoch 13. Iteration 00100, Avg-Loss: 1.5069, Accuracy: 0.4830\n",
      "Train-epoch 13. Iteration 00200, Avg-Loss: 1.5065, Accuracy: 0.4748\n",
      "Train-epoch 13. Iteration 00300, Avg-Loss: 1.5046, Accuracy: 0.4725\n",
      "Train-epoch 13. Iteration 00400, Avg-Loss: 1.5045, Accuracy: 0.4728\n",
      "Train-epoch 13. Iteration 00500, Avg-Loss: 1.5011, Accuracy: 0.4732\n",
      "Validation-epoch 13. Avg-Loss: 1.5000, Accuracy: 47.2700\n",
      "--------------------------------------------------\n",
      "Train-epoch 14. Iteration 00100, Avg-Loss: 1.4725, Accuracy: 0.4815\n",
      "Train-epoch 14. Iteration 00200, Avg-Loss: 1.4779, Accuracy: 0.4811\n",
      "Train-epoch 14. Iteration 00300, Avg-Loss: 1.4769, Accuracy: 0.4819\n",
      "Train-epoch 14. Iteration 00400, Avg-Loss: 1.4817, Accuracy: 0.4811\n",
      "Train-epoch 14. Iteration 00500, Avg-Loss: 1.4855, Accuracy: 0.4804\n",
      "Validation-epoch 14. Avg-Loss: 1.5318, Accuracy: 45.9500\n",
      "--------------------------------------------------\n",
      "Train-epoch 15. Iteration 00100, Avg-Loss: 1.4621, Accuracy: 0.4947\n",
      "Train-epoch 15. Iteration 00200, Avg-Loss: 1.4587, Accuracy: 0.4919\n",
      "Train-epoch 15. Iteration 00300, Avg-Loss: 1.4645, Accuracy: 0.4858\n",
      "Train-epoch 15. Iteration 00400, Avg-Loss: 1.4709, Accuracy: 0.4821\n",
      "Train-epoch 15. Iteration 00500, Avg-Loss: 1.4695, Accuracy: 0.4825\n",
      "Validation-epoch 15. Avg-Loss: 1.5017, Accuracy: 46.6800\n",
      "--------------------------------------------------\n",
      "Train-epoch 16. Iteration 00100, Avg-Loss: 1.4563, Accuracy: 0.4921\n",
      "Train-epoch 16. Iteration 00200, Avg-Loss: 1.4568, Accuracy: 0.4881\n",
      "Train-epoch 16. Iteration 00300, Avg-Loss: 1.4572, Accuracy: 0.4882\n",
      "Train-epoch 16. Iteration 00400, Avg-Loss: 1.4529, Accuracy: 0.4900\n",
      "Train-epoch 16. Iteration 00500, Avg-Loss: 1.4543, Accuracy: 0.4883\n",
      "Validation-epoch 16. Avg-Loss: 1.4794, Accuracy: 47.6300\n",
      "--------------------------------------------------\n",
      "Train-epoch 17. Iteration 00100, Avg-Loss: 1.4378, Accuracy: 0.5005\n",
      "Train-epoch 17. Iteration 00200, Avg-Loss: 1.4405, Accuracy: 0.4960\n",
      "Train-epoch 17. Iteration 00300, Avg-Loss: 1.4396, Accuracy: 0.4936\n",
      "Train-epoch 17. Iteration 00400, Avg-Loss: 1.4392, Accuracy: 0.4933\n",
      "Train-epoch 17. Iteration 00500, Avg-Loss: 1.4384, Accuracy: 0.4929\n",
      "Validation-epoch 17. Avg-Loss: 1.4764, Accuracy: 48.0200\n",
      "--------------------------------------------------\n",
      "Train-epoch 18. Iteration 00100, Avg-Loss: 1.4231, Accuracy: 0.5057\n",
      "Train-epoch 18. Iteration 00200, Avg-Loss: 1.4253, Accuracy: 0.5030\n",
      "Train-epoch 18. Iteration 00300, Avg-Loss: 1.4213, Accuracy: 0.5017\n",
      "Train-epoch 18. Iteration 00400, Avg-Loss: 1.4212, Accuracy: 0.5015\n",
      "Train-epoch 18. Iteration 00500, Avg-Loss: 1.4242, Accuracy: 0.5014\n",
      "Validation-epoch 18. Avg-Loss: 1.4469, Accuracy: 48.4700\n",
      "--------------------------------------------------\n",
      "Train-epoch 19. Iteration 00100, Avg-Loss: 1.4264, Accuracy: 0.5061\n",
      "Train-epoch 19. Iteration 00200, Avg-Loss: 1.4237, Accuracy: 0.5017\n",
      "Train-epoch 19. Iteration 00300, Avg-Loss: 1.4172, Accuracy: 0.5021\n",
      "Train-epoch 19. Iteration 00400, Avg-Loss: 1.4183, Accuracy: 0.5007\n",
      "Train-epoch 19. Iteration 00500, Avg-Loss: 1.4167, Accuracy: 0.5017\n",
      "Validation-epoch 19. Avg-Loss: 1.4464, Accuracy: 48.5500\n",
      "--------------------------------------------------\n",
      "Train-epoch 20. Iteration 00100, Avg-Loss: 1.3976, Accuracy: 0.5132\n",
      "Train-epoch 20. Iteration 00200, Avg-Loss: 1.4041, Accuracy: 0.5092\n",
      "Train-epoch 20. Iteration 00300, Avg-Loss: 1.4023, Accuracy: 0.5082\n",
      "Train-epoch 20. Iteration 00400, Avg-Loss: 1.3979, Accuracy: 0.5093\n",
      "Train-epoch 20. Iteration 00500, Avg-Loss: 1.3996, Accuracy: 0.5080\n",
      "Validation-epoch 20. Avg-Loss: 1.4563, Accuracy: 48.7500\n",
      "--------------------------------------------------\n",
      "Train-epoch 21. Iteration 00100, Avg-Loss: 1.3889, Accuracy: 0.5126\n",
      "Train-epoch 21. Iteration 00200, Avg-Loss: 1.3918, Accuracy: 0.5121\n",
      "Train-epoch 21. Iteration 00300, Avg-Loss: 1.3937, Accuracy: 0.5106\n",
      "Train-epoch 21. Iteration 00400, Avg-Loss: 1.3963, Accuracy: 0.5099\n",
      "Train-epoch 21. Iteration 00500, Avg-Loss: 1.3885, Accuracy: 0.5127\n",
      "Validation-epoch 21. Avg-Loss: 1.4303, Accuracy: 49.0400\n",
      "--------------------------------------------------\n",
      "Train-epoch 22. Iteration 00100, Avg-Loss: 1.3765, Accuracy: 0.5141\n",
      "Train-epoch 22. Iteration 00200, Avg-Loss: 1.3726, Accuracy: 0.5144\n",
      "Train-epoch 22. Iteration 00300, Avg-Loss: 1.3683, Accuracy: 0.5173\n",
      "Train-epoch 22. Iteration 00400, Avg-Loss: 1.3713, Accuracy: 0.5165\n",
      "Train-epoch 22. Iteration 00500, Avg-Loss: 1.3739, Accuracy: 0.5168\n",
      "Validation-epoch 22. Avg-Loss: 1.4178, Accuracy: 49.9300\n",
      "--------------------------------------------------\n",
      "Train-epoch 23. Iteration 00100, Avg-Loss: 1.3654, Accuracy: 0.5268\n",
      "Train-epoch 23. Iteration 00200, Avg-Loss: 1.3648, Accuracy: 0.5242\n",
      "Train-epoch 23. Iteration 00300, Avg-Loss: 1.3647, Accuracy: 0.5219\n",
      "Train-epoch 23. Iteration 00400, Avg-Loss: 1.3630, Accuracy: 0.5212\n",
      "Train-epoch 23. Iteration 00500, Avg-Loss: 1.3639, Accuracy: 0.5206\n",
      "Validation-epoch 23. Avg-Loss: 1.4161, Accuracy: 49.4400\n",
      "--------------------------------------------------\n",
      "Train-epoch 24. Iteration 00100, Avg-Loss: 1.3571, Accuracy: 0.5335\n",
      "Train-epoch 24. Iteration 00200, Avg-Loss: 1.3518, Accuracy: 0.5318\n",
      "Train-epoch 24. Iteration 00300, Avg-Loss: 1.3549, Accuracy: 0.5277\n",
      "Train-epoch 24. Iteration 00400, Avg-Loss: 1.3523, Accuracy: 0.5269\n",
      "Train-epoch 24. Iteration 00500, Avg-Loss: 1.3551, Accuracy: 0.5263\n",
      "Validation-epoch 24. Avg-Loss: 1.4245, Accuracy: 49.1100\n",
      "--------------------------------------------------\n",
      "Train-epoch 25. Iteration 00100, Avg-Loss: 1.3343, Accuracy: 0.5348\n",
      "Train-epoch 25. Iteration 00200, Avg-Loss: 1.3333, Accuracy: 0.5312\n",
      "Train-epoch 25. Iteration 00300, Avg-Loss: 1.3374, Accuracy: 0.5295\n",
      "Train-epoch 25. Iteration 00400, Avg-Loss: 1.3397, Accuracy: 0.5294\n",
      "Train-epoch 25. Iteration 00500, Avg-Loss: 1.3427, Accuracy: 0.5287\n",
      "Validation-epoch 25. Avg-Loss: 1.4310, Accuracy: 49.9200\n",
      "--------------------------------------------------\n",
      "Train-epoch 26. Iteration 00100, Avg-Loss: 1.3329, Accuracy: 0.5359\n",
      "Train-epoch 26. Iteration 00200, Avg-Loss: 1.3265, Accuracy: 0.5359\n",
      "Train-epoch 26. Iteration 00300, Avg-Loss: 1.3288, Accuracy: 0.5341\n",
      "Train-epoch 26. Iteration 00400, Avg-Loss: 1.3299, Accuracy: 0.5336\n",
      "Train-epoch 26. Iteration 00500, Avg-Loss: 1.3292, Accuracy: 0.5331\n",
      "Validation-epoch 26. Avg-Loss: 1.4313, Accuracy: 49.1100\n",
      "--------------------------------------------------\n",
      "Train-epoch 27. Iteration 00100, Avg-Loss: 1.3234, Accuracy: 0.5435\n",
      "Train-epoch 27. Iteration 00200, Avg-Loss: 1.3167, Accuracy: 0.5421\n",
      "Train-epoch 27. Iteration 00300, Avg-Loss: 1.3212, Accuracy: 0.5392\n",
      "Train-epoch 27. Iteration 00400, Avg-Loss: 1.3208, Accuracy: 0.5372\n",
      "Train-epoch 27. Iteration 00500, Avg-Loss: 1.3219, Accuracy: 0.5372\n",
      "Validation-epoch 27. Avg-Loss: 1.4264, Accuracy: 49.4900\n",
      "--------------------------------------------------\n",
      "Train-epoch 28. Iteration 00100, Avg-Loss: 1.3179, Accuracy: 0.5415\n",
      "Train-epoch 28. Iteration 00200, Avg-Loss: 1.3138, Accuracy: 0.5404\n",
      "Train-epoch 28. Iteration 00300, Avg-Loss: 1.3123, Accuracy: 0.5415\n",
      "Train-epoch 28. Iteration 00400, Avg-Loss: 1.3122, Accuracy: 0.5409\n",
      "Train-epoch 28. Iteration 00500, Avg-Loss: 1.3097, Accuracy: 0.5411\n",
      "Validation-epoch 28. Avg-Loss: 1.3922, Accuracy: 50.8400\n",
      "--------------------------------------------------\n",
      "Train-epoch 29. Iteration 00100, Avg-Loss: 1.2983, Accuracy: 0.5471\n",
      "Train-epoch 29. Iteration 00200, Avg-Loss: 1.3002, Accuracy: 0.5486\n",
      "Train-epoch 29. Iteration 00300, Avg-Loss: 1.3032, Accuracy: 0.5461\n",
      "Train-epoch 29. Iteration 00400, Avg-Loss: 1.3016, Accuracy: 0.5462\n",
      "Train-epoch 29. Iteration 00500, Avg-Loss: 1.2987, Accuracy: 0.5466\n",
      "Validation-epoch 29. Avg-Loss: 1.3830, Accuracy: 50.9600\n",
      "--------------------------------------------------\n",
      "Train-epoch 30. Iteration 00100, Avg-Loss: 1.2871, Accuracy: 0.5553\n",
      "Train-epoch 30. Iteration 00200, Avg-Loss: 1.2924, Accuracy: 0.5504\n",
      "Train-epoch 30. Iteration 00300, Avg-Loss: 1.2888, Accuracy: 0.5508\n",
      "Train-epoch 30. Iteration 00400, Avg-Loss: 1.2900, Accuracy: 0.5490\n",
      "Train-epoch 30. Iteration 00500, Avg-Loss: 1.2893, Accuracy: 0.5495\n",
      "Validation-epoch 30. Avg-Loss: 1.3740, Accuracy: 51.7200\n",
      "--------------------------------------------------\n",
      "Train-epoch 31. Iteration 00100, Avg-Loss: 1.2730, Accuracy: 0.5583\n",
      "Train-epoch 31. Iteration 00200, Avg-Loss: 1.2781, Accuracy: 0.5554\n",
      "Train-epoch 31. Iteration 00300, Avg-Loss: 1.2799, Accuracy: 0.5551\n",
      "Train-epoch 31. Iteration 00400, Avg-Loss: 1.2765, Accuracy: 0.5554\n",
      "Train-epoch 31. Iteration 00500, Avg-Loss: 1.2791, Accuracy: 0.5546\n",
      "Validation-epoch 31. Avg-Loss: 1.3889, Accuracy: 50.4000\n",
      "--------------------------------------------------\n",
      "Train-epoch 32. Iteration 00100, Avg-Loss: 1.2800, Accuracy: 0.5591\n",
      "Train-epoch 32. Iteration 00200, Avg-Loss: 1.2745, Accuracy: 0.5568\n",
      "Train-epoch 32. Iteration 00300, Avg-Loss: 1.2746, Accuracy: 0.5562\n",
      "Train-epoch 32. Iteration 00400, Avg-Loss: 1.2753, Accuracy: 0.5562\n",
      "Train-epoch 32. Iteration 00500, Avg-Loss: 1.2731, Accuracy: 0.5568\n",
      "Validation-epoch 32. Avg-Loss: 1.3726, Accuracy: 50.9800\n",
      "--------------------------------------------------\n",
      "Train-epoch 33. Iteration 00100, Avg-Loss: 1.2655, Accuracy: 0.5643\n",
      "Train-epoch 33. Iteration 00200, Avg-Loss: 1.2687, Accuracy: 0.5571\n",
      "Train-epoch 33. Iteration 00300, Avg-Loss: 1.2687, Accuracy: 0.5575\n",
      "Train-epoch 33. Iteration 00400, Avg-Loss: 1.2669, Accuracy: 0.5597\n",
      "Train-epoch 33. Iteration 00500, Avg-Loss: 1.2631, Accuracy: 0.5597\n",
      "Validation-epoch 33. Avg-Loss: 1.3768, Accuracy: 51.4900\n",
      "--------------------------------------------------\n",
      "Train-epoch 34. Iteration 00100, Avg-Loss: 1.2571, Accuracy: 0.5665\n",
      "Train-epoch 34. Iteration 00200, Avg-Loss: 1.2525, Accuracy: 0.5655\n",
      "Train-epoch 34. Iteration 00300, Avg-Loss: 1.2515, Accuracy: 0.5656\n",
      "Train-epoch 34. Iteration 00400, Avg-Loss: 1.2518, Accuracy: 0.5668\n",
      "Train-epoch 34. Iteration 00500, Avg-Loss: 1.2526, Accuracy: 0.5644\n",
      "Validation-epoch 34. Avg-Loss: 1.3687, Accuracy: 51.3000\n",
      "--------------------------------------------------\n",
      "Train-epoch 35. Iteration 00100, Avg-Loss: 1.2361, Accuracy: 0.5641\n",
      "Train-epoch 35. Iteration 00200, Avg-Loss: 1.2445, Accuracy: 0.5650\n",
      "Train-epoch 35. Iteration 00300, Avg-Loss: 1.2425, Accuracy: 0.5667\n",
      "Train-epoch 35. Iteration 00400, Avg-Loss: 1.2435, Accuracy: 0.5654\n",
      "Train-epoch 35. Iteration 00500, Avg-Loss: 1.2434, Accuracy: 0.5662\n",
      "Validation-epoch 35. Avg-Loss: 1.3456, Accuracy: 52.8800\n",
      "--------------------------------------------------\n",
      "Train-epoch 36. Iteration 00100, Avg-Loss: 1.2328, Accuracy: 0.5818\n",
      "Train-epoch 36. Iteration 00200, Avg-Loss: 1.2293, Accuracy: 0.5740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 36. Iteration 00300, Avg-Loss: 1.2352, Accuracy: 0.5708\n",
      "Train-epoch 36. Iteration 00400, Avg-Loss: 1.2342, Accuracy: 0.5712\n",
      "Train-epoch 36. Iteration 00500, Avg-Loss: 1.2370, Accuracy: 0.5692\n",
      "Validation-epoch 36. Avg-Loss: 1.3541, Accuracy: 52.5700\n",
      "--------------------------------------------------\n",
      "Train-epoch 37. Iteration 00100, Avg-Loss: 1.2238, Accuracy: 0.5803\n",
      "Train-epoch 37. Iteration 00200, Avg-Loss: 1.2237, Accuracy: 0.5741\n",
      "Train-epoch 37. Iteration 00300, Avg-Loss: 1.2255, Accuracy: 0.5734\n",
      "Train-epoch 37. Iteration 00400, Avg-Loss: 1.2276, Accuracy: 0.5716\n",
      "Train-epoch 37. Iteration 00500, Avg-Loss: 1.2290, Accuracy: 0.5700\n",
      "Validation-epoch 37. Avg-Loss: 1.3433, Accuracy: 52.0800\n",
      "--------------------------------------------------\n",
      "Train-epoch 38. Iteration 00100, Avg-Loss: 1.2198, Accuracy: 0.5812\n",
      "Train-epoch 38. Iteration 00200, Avg-Loss: 1.2183, Accuracy: 0.5764\n",
      "Train-epoch 38. Iteration 00300, Avg-Loss: 1.2199, Accuracy: 0.5767\n",
      "Train-epoch 38. Iteration 00400, Avg-Loss: 1.2176, Accuracy: 0.5762\n",
      "Train-epoch 38. Iteration 00500, Avg-Loss: 1.2187, Accuracy: 0.5746\n",
      "Validation-epoch 38. Avg-Loss: 1.3300, Accuracy: 53.8600\n",
      "--------------------------------------------------\n",
      "Train-epoch 39. Iteration 00100, Avg-Loss: 1.2055, Accuracy: 0.5802\n",
      "Train-epoch 39. Iteration 00200, Avg-Loss: 1.2091, Accuracy: 0.5777\n",
      "Train-epoch 39. Iteration 00300, Avg-Loss: 1.2030, Accuracy: 0.5786\n",
      "Train-epoch 39. Iteration 00400, Avg-Loss: 1.2090, Accuracy: 0.5771\n",
      "Train-epoch 39. Iteration 00500, Avg-Loss: 1.2125, Accuracy: 0.5773\n",
      "Validation-epoch 39. Avg-Loss: 1.3770, Accuracy: 51.4500\n",
      "--------------------------------------------------\n",
      "Train-epoch 40. Iteration 00100, Avg-Loss: 1.2000, Accuracy: 0.5834\n",
      "Train-epoch 40. Iteration 00200, Avg-Loss: 1.2012, Accuracy: 0.5796\n",
      "Train-epoch 40. Iteration 00300, Avg-Loss: 1.2032, Accuracy: 0.5814\n",
      "Train-epoch 40. Iteration 00400, Avg-Loss: 1.2009, Accuracy: 0.5813\n",
      "Train-epoch 40. Iteration 00500, Avg-Loss: 1.2039, Accuracy: 0.5791\n",
      "Validation-epoch 40. Avg-Loss: 1.3340, Accuracy: 52.9400\n",
      "--------------------------------------------------\n",
      "Train-epoch 41. Iteration 00100, Avg-Loss: 1.1691, Accuracy: 0.6028\n",
      "Train-epoch 41. Iteration 00200, Avg-Loss: 1.1799, Accuracy: 0.5920\n",
      "Train-epoch 41. Iteration 00300, Avg-Loss: 1.1863, Accuracy: 0.5885\n",
      "Train-epoch 41. Iteration 00400, Avg-Loss: 1.1940, Accuracy: 0.5842\n",
      "Train-epoch 41. Iteration 00500, Avg-Loss: 1.1939, Accuracy: 0.5831\n",
      "Validation-epoch 41. Avg-Loss: 1.3581, Accuracy: 51.3300\n",
      "--------------------------------------------------\n",
      "Train-epoch 42. Iteration 00100, Avg-Loss: 1.1875, Accuracy: 0.5877\n",
      "Train-epoch 42. Iteration 00200, Avg-Loss: 1.1899, Accuracy: 0.5851\n",
      "Train-epoch 42. Iteration 00300, Avg-Loss: 1.1877, Accuracy: 0.5844\n",
      "Train-epoch 42. Iteration 00400, Avg-Loss: 1.1866, Accuracy: 0.5852\n",
      "Train-epoch 42. Iteration 00500, Avg-Loss: 1.1875, Accuracy: 0.5842\n",
      "Validation-epoch 42. Avg-Loss: 1.3280, Accuracy: 52.7000\n",
      "--------------------------------------------------\n",
      "Train-epoch 43. Iteration 00100, Avg-Loss: 1.1876, Accuracy: 0.5854\n",
      "Train-epoch 43. Iteration 00200, Avg-Loss: 1.1801, Accuracy: 0.5897\n",
      "Train-epoch 43. Iteration 00300, Avg-Loss: 1.1788, Accuracy: 0.5897\n",
      "Train-epoch 43. Iteration 00400, Avg-Loss: 1.1761, Accuracy: 0.5903\n",
      "Train-epoch 43. Iteration 00500, Avg-Loss: 1.1795, Accuracy: 0.5895\n",
      "Validation-epoch 43. Avg-Loss: 1.3414, Accuracy: 52.1900\n",
      "--------------------------------------------------\n",
      "Train-epoch 44. Iteration 00100, Avg-Loss: 1.1654, Accuracy: 0.5973\n",
      "Train-epoch 44. Iteration 00200, Avg-Loss: 1.1670, Accuracy: 0.5956\n",
      "Train-epoch 44. Iteration 00300, Avg-Loss: 1.1689, Accuracy: 0.5932\n",
      "Train-epoch 44. Iteration 00400, Avg-Loss: 1.1694, Accuracy: 0.5933\n",
      "Train-epoch 44. Iteration 00500, Avg-Loss: 1.1717, Accuracy: 0.5930\n",
      "Validation-epoch 44. Avg-Loss: 1.3663, Accuracy: 52.3600\n",
      "--------------------------------------------------\n",
      "Train-epoch 45. Iteration 00100, Avg-Loss: 1.1660, Accuracy: 0.5988\n",
      "Train-epoch 45. Iteration 00200, Avg-Loss: 1.1642, Accuracy: 0.5980\n",
      "Train-epoch 45. Iteration 00300, Avg-Loss: 1.1656, Accuracy: 0.5970\n",
      "Train-epoch 45. Iteration 00400, Avg-Loss: 1.1645, Accuracy: 0.5953\n",
      "Train-epoch 45. Iteration 00500, Avg-Loss: 1.1679, Accuracy: 0.5932\n",
      "Validation-epoch 45. Avg-Loss: 1.4107, Accuracy: 49.9300\n",
      "--------------------------------------------------\n",
      "Train-epoch 46. Iteration 00100, Avg-Loss: 1.1540, Accuracy: 0.6047\n",
      "Train-epoch 46. Iteration 00200, Avg-Loss: 1.1499, Accuracy: 0.6038\n",
      "Train-epoch 46. Iteration 00300, Avg-Loss: 1.1598, Accuracy: 0.5990\n",
      "Train-epoch 46. Iteration 00400, Avg-Loss: 1.1584, Accuracy: 0.5983\n",
      "Train-epoch 46. Iteration 00500, Avg-Loss: 1.1642, Accuracy: 0.5968\n",
      "Validation-epoch 46. Avg-Loss: 1.3407, Accuracy: 52.4000\n",
      "--------------------------------------------------\n",
      "Train-epoch 47. Iteration 00100, Avg-Loss: 1.1431, Accuracy: 0.6103\n",
      "Train-epoch 47. Iteration 00200, Avg-Loss: 1.1457, Accuracy: 0.6031\n",
      "Train-epoch 47. Iteration 00300, Avg-Loss: 1.1485, Accuracy: 0.6018\n",
      "Train-epoch 47. Iteration 00400, Avg-Loss: 1.1509, Accuracy: 0.5988\n",
      "Train-epoch 47. Iteration 00500, Avg-Loss: 1.1545, Accuracy: 0.5974\n",
      "Validation-epoch 47. Avg-Loss: 1.3132, Accuracy: 53.8400\n",
      "--------------------------------------------------\n",
      "Train-epoch 48. Iteration 00100, Avg-Loss: 1.1452, Accuracy: 0.6023\n",
      "Train-epoch 48. Iteration 00200, Avg-Loss: 1.1458, Accuracy: 0.6004\n",
      "Train-epoch 48. Iteration 00300, Avg-Loss: 1.1433, Accuracy: 0.6015\n",
      "Train-epoch 48. Iteration 00400, Avg-Loss: 1.1422, Accuracy: 0.6017\n",
      "Train-epoch 48. Iteration 00500, Avg-Loss: 1.1449, Accuracy: 0.6003\n",
      "Validation-epoch 48. Avg-Loss: 1.3072, Accuracy: 54.0600\n",
      "--------------------------------------------------\n",
      "Train-epoch 49. Iteration 00100, Avg-Loss: 1.1319, Accuracy: 0.6085\n",
      "Train-epoch 49. Iteration 00200, Avg-Loss: 1.1362, Accuracy: 0.6054\n",
      "Train-epoch 49. Iteration 00300, Avg-Loss: 1.1410, Accuracy: 0.6041\n",
      "Train-epoch 49. Iteration 00400, Avg-Loss: 1.1373, Accuracy: 0.6049\n",
      "Train-epoch 49. Iteration 00500, Avg-Loss: 1.1383, Accuracy: 0.6037\n",
      "Validation-epoch 49. Avg-Loss: 1.3739, Accuracy: 51.0900\n",
      "--------------------------------------------------\n",
      "Train-epoch 50. Iteration 00100, Avg-Loss: 1.1482, Accuracy: 0.6052\n",
      "Train-epoch 50. Iteration 00200, Avg-Loss: 1.1344, Accuracy: 0.6092\n",
      "Train-epoch 50. Iteration 00300, Avg-Loss: 1.1342, Accuracy: 0.6082\n",
      "Train-epoch 50. Iteration 00400, Avg-Loss: 1.1313, Accuracy: 0.6093\n",
      "Train-epoch 50. Iteration 00500, Avg-Loss: 1.1290, Accuracy: 0.6100\n",
      "Validation-epoch 50. Avg-Loss: 1.3533, Accuracy: 51.8900\n",
      "--------------------------------------------------\n",
      "Train-epoch 51. Iteration 00100, Avg-Loss: 1.1274, Accuracy: 0.6183\n",
      "Train-epoch 51. Iteration 00200, Avg-Loss: 1.1298, Accuracy: 0.6096\n",
      "Train-epoch 51. Iteration 00300, Avg-Loss: 1.1307, Accuracy: 0.6090\n",
      "Train-epoch 51. Iteration 00400, Avg-Loss: 1.1279, Accuracy: 0.6094\n",
      "Train-epoch 51. Iteration 00500, Avg-Loss: 1.1234, Accuracy: 0.6094\n",
      "Validation-epoch 51. Avg-Loss: 1.3227, Accuracy: 53.3500\n",
      "--------------------------------------------------\n",
      "Train-epoch 52. Iteration 00100, Avg-Loss: 1.1074, Accuracy: 0.6179\n",
      "Train-epoch 52. Iteration 00200, Avg-Loss: 1.1010, Accuracy: 0.6181\n",
      "Train-epoch 52. Iteration 00300, Avg-Loss: 1.1045, Accuracy: 0.6169\n",
      "Train-epoch 52. Iteration 00400, Avg-Loss: 1.1131, Accuracy: 0.6123\n",
      "Train-epoch 52. Iteration 00500, Avg-Loss: 1.1138, Accuracy: 0.6118\n",
      "Validation-epoch 52. Avg-Loss: 1.3431, Accuracy: 53.1100\n",
      "--------------------------------------------------\n",
      "Train-epoch 53. Iteration 00100, Avg-Loss: 1.1076, Accuracy: 0.6246\n",
      "Train-epoch 53. Iteration 00200, Avg-Loss: 1.1104, Accuracy: 0.6183\n",
      "Train-epoch 53. Iteration 00300, Avg-Loss: 1.1082, Accuracy: 0.6167\n",
      "Train-epoch 53. Iteration 00400, Avg-Loss: 1.1104, Accuracy: 0.6147\n",
      "Train-epoch 53. Iteration 00500, Avg-Loss: 1.1134, Accuracy: 0.6132\n",
      "Validation-epoch 53. Avg-Loss: 1.3445, Accuracy: 52.3100\n",
      "--------------------------------------------------\n",
      "Train-epoch 54. Iteration 00100, Avg-Loss: 1.0904, Accuracy: 0.6277\n",
      "Train-epoch 54. Iteration 00200, Avg-Loss: 1.0966, Accuracy: 0.6216\n",
      "Train-epoch 54. Iteration 00300, Avg-Loss: 1.1047, Accuracy: 0.6164\n",
      "Train-epoch 54. Iteration 00400, Avg-Loss: 1.1034, Accuracy: 0.6180\n",
      "Train-epoch 54. Iteration 00500, Avg-Loss: 1.1042, Accuracy: 0.6168\n",
      "Validation-epoch 54. Avg-Loss: 1.3897, Accuracy: 51.1700\n",
      "--------------------------------------------------\n",
      "Train-epoch 55. Iteration 00100, Avg-Loss: 1.0881, Accuracy: 0.6306\n",
      "Train-epoch 55. Iteration 00200, Avg-Loss: 1.0829, Accuracy: 0.6279\n",
      "Train-epoch 55. Iteration 00300, Avg-Loss: 1.0882, Accuracy: 0.6231\n",
      "Train-epoch 55. Iteration 00400, Avg-Loss: 1.0940, Accuracy: 0.6215\n",
      "Train-epoch 55. Iteration 00500, Avg-Loss: 1.0951, Accuracy: 0.6205\n",
      "Validation-epoch 55. Avg-Loss: 1.3375, Accuracy: 52.2700\n",
      "--------------------------------------------------\n",
      "Train-epoch 56. Iteration 00100, Avg-Loss: 1.0802, Accuracy: 0.6333\n",
      "Train-epoch 56. Iteration 00200, Avg-Loss: 1.0880, Accuracy: 0.6252\n",
      "Train-epoch 56. Iteration 00300, Avg-Loss: 1.0913, Accuracy: 0.6223\n",
      "Train-epoch 56. Iteration 00400, Avg-Loss: 1.0893, Accuracy: 0.6226\n",
      "Train-epoch 56. Iteration 00500, Avg-Loss: 1.0897, Accuracy: 0.6214\n",
      "Validation-epoch 56. Avg-Loss: 1.3132, Accuracy: 53.9700\n",
      "--------------------------------------------------\n",
      "Train-epoch 57. Iteration 00100, Avg-Loss: 1.0664, Accuracy: 0.6327\n",
      "Train-epoch 57. Iteration 00200, Avg-Loss: 1.0852, Accuracy: 0.6239\n",
      "Train-epoch 57. Iteration 00300, Avg-Loss: 1.0852, Accuracy: 0.6246\n",
      "Train-epoch 57. Iteration 00400, Avg-Loss: 1.0826, Accuracy: 0.6250\n",
      "Train-epoch 57. Iteration 00500, Avg-Loss: 1.0816, Accuracy: 0.6240\n",
      "Validation-epoch 57. Avg-Loss: 1.3164, Accuracy: 53.7500\n",
      "--------------------------------------------------\n",
      "Train-epoch 58. Iteration 00100, Avg-Loss: 1.0678, Accuracy: 0.6371\n",
      "Train-epoch 58. Iteration 00200, Avg-Loss: 1.0777, Accuracy: 0.6282\n",
      "Train-epoch 58. Iteration 00300, Avg-Loss: 1.0788, Accuracy: 0.6243\n",
      "Train-epoch 58. Iteration 00400, Avg-Loss: 1.0758, Accuracy: 0.6260\n",
      "Train-epoch 58. Iteration 00500, Avg-Loss: 1.0772, Accuracy: 0.6257\n",
      "Validation-epoch 58. Avg-Loss: 1.2989, Accuracy: 54.4000\n",
      "--------------------------------------------------\n",
      "Train-epoch 59. Iteration 00100, Avg-Loss: 1.0641, Accuracy: 0.6370\n",
      "Train-epoch 59. Iteration 00200, Avg-Loss: 1.0706, Accuracy: 0.6311\n",
      "Train-epoch 59. Iteration 00300, Avg-Loss: 1.0648, Accuracy: 0.6344\n",
      "Train-epoch 59. Iteration 00400, Avg-Loss: 1.0679, Accuracy: 0.6318\n",
      "Train-epoch 59. Iteration 00500, Avg-Loss: 1.0694, Accuracy: 0.6313\n",
      "Validation-epoch 59. Avg-Loss: 1.3036, Accuracy: 54.0300\n",
      "--------------------------------------------------\n",
      "Train-epoch 60. Iteration 00100, Avg-Loss: 1.0731, Accuracy: 0.6346\n",
      "Train-epoch 60. Iteration 00200, Avg-Loss: 1.0722, Accuracy: 0.6313\n",
      "Train-epoch 60. Iteration 00300, Avg-Loss: 1.0692, Accuracy: 0.6315\n",
      "Train-epoch 60. Iteration 00400, Avg-Loss: 1.0663, Accuracy: 0.6314\n",
      "Train-epoch 60. Iteration 00500, Avg-Loss: 1.0646, Accuracy: 0.6307\n",
      "Validation-epoch 60. Avg-Loss: 1.3765, Accuracy: 51.9200\n",
      "--------------------------------------------------\n",
      "Train-epoch 61. Iteration 00100, Avg-Loss: 1.0616, Accuracy: 0.6367\n",
      "Train-epoch 61. Iteration 00200, Avg-Loss: 1.0440, Accuracy: 0.6391\n",
      "Train-epoch 61. Iteration 00300, Avg-Loss: 1.0539, Accuracy: 0.6357\n",
      "Train-epoch 61. Iteration 00400, Avg-Loss: 1.0567, Accuracy: 0.6352\n",
      "Train-epoch 61. Iteration 00500, Avg-Loss: 1.0562, Accuracy: 0.6342\n",
      "Validation-epoch 61. Avg-Loss: 1.3008, Accuracy: 54.1900\n",
      "--------------------------------------------------\n",
      "Train-epoch 62. Iteration 00100, Avg-Loss: 1.0235, Accuracy: 0.6470\n",
      "Train-epoch 62. Iteration 00200, Avg-Loss: 1.0411, Accuracy: 0.6397\n",
      "Train-epoch 62. Iteration 00300, Avg-Loss: 1.0448, Accuracy: 0.6388\n",
      "Train-epoch 62. Iteration 00400, Avg-Loss: 1.0440, Accuracy: 0.6380\n",
      "Train-epoch 62. Iteration 00500, Avg-Loss: 1.0485, Accuracy: 0.6357\n",
      "Validation-epoch 62. Avg-Loss: 1.3754, Accuracy: 51.2400\n",
      "--------------------------------------------------\n",
      "Train-epoch 63. Iteration 00100, Avg-Loss: 1.0266, Accuracy: 0.6525\n",
      "Train-epoch 63. Iteration 00200, Avg-Loss: 1.0360, Accuracy: 0.6408\n",
      "Train-epoch 63. Iteration 00300, Avg-Loss: 1.0366, Accuracy: 0.6411\n",
      "Train-epoch 63. Iteration 00400, Avg-Loss: 1.0407, Accuracy: 0.6391\n",
      "Train-epoch 63. Iteration 00500, Avg-Loss: 1.0425, Accuracy: 0.6385\n",
      "Validation-epoch 63. Avg-Loss: 1.3045, Accuracy: 54.4200\n",
      "--------------------------------------------------\n",
      "Train-epoch 64. Iteration 00100, Avg-Loss: 1.0284, Accuracy: 0.6493\n",
      "Train-epoch 64. Iteration 00200, Avg-Loss: 1.0411, Accuracy: 0.6434\n",
      "Train-epoch 64. Iteration 00300, Avg-Loss: 1.0383, Accuracy: 0.6432\n",
      "Train-epoch 64. Iteration 00400, Avg-Loss: 1.0363, Accuracy: 0.6430\n",
      "Train-epoch 64. Iteration 00500, Avg-Loss: 1.0349, Accuracy: 0.6438\n",
      "Validation-epoch 64. Avg-Loss: 1.3193, Accuracy: 53.7800\n",
      "--------------------------------------------------\n",
      "Train-epoch 65. Iteration 00100, Avg-Loss: 1.0272, Accuracy: 0.6506\n",
      "Train-epoch 65. Iteration 00200, Avg-Loss: 1.0349, Accuracy: 0.6464\n",
      "Train-epoch 65. Iteration 00300, Avg-Loss: 1.0321, Accuracy: 0.6460\n",
      "Train-epoch 65. Iteration 00400, Avg-Loss: 1.0305, Accuracy: 0.6438\n",
      "Train-epoch 65. Iteration 00500, Avg-Loss: 1.0289, Accuracy: 0.6444\n",
      "Validation-epoch 65. Avg-Loss: 1.2852, Accuracy: 55.2100\n",
      "--------------------------------------------------\n",
      "Train-epoch 66. Iteration 00100, Avg-Loss: 1.0378, Accuracy: 0.6505\n",
      "Train-epoch 66. Iteration 00200, Avg-Loss: 1.0317, Accuracy: 0.6468\n",
      "Train-epoch 66. Iteration 00300, Avg-Loss: 1.0295, Accuracy: 0.6478\n",
      "Train-epoch 66. Iteration 00400, Avg-Loss: 1.0271, Accuracy: 0.6471\n",
      "Train-epoch 66. Iteration 00500, Avg-Loss: 1.0261, Accuracy: 0.6458\n",
      "Validation-epoch 66. Avg-Loss: 1.2891, Accuracy: 54.7500\n",
      "--------------------------------------------------\n",
      "Train-epoch 67. Iteration 00100, Avg-Loss: 1.0159, Accuracy: 0.6570\n",
      "Train-epoch 67. Iteration 00200, Avg-Loss: 1.0243, Accuracy: 0.6498\n",
      "Train-epoch 67. Iteration 00300, Avg-Loss: 1.0240, Accuracy: 0.6459\n",
      "Train-epoch 67. Iteration 00400, Avg-Loss: 1.0218, Accuracy: 0.6458\n",
      "Train-epoch 67. Iteration 00500, Avg-Loss: 1.0198, Accuracy: 0.6468\n",
      "Validation-epoch 67. Avg-Loss: 1.2982, Accuracy: 54.3500\n",
      "--------------------------------------------------\n",
      "Train-epoch 68. Iteration 00100, Avg-Loss: 0.9984, Accuracy: 0.6577\n",
      "Train-epoch 68. Iteration 00200, Avg-Loss: 1.0134, Accuracy: 0.6511\n",
      "Train-epoch 68. Iteration 00300, Avg-Loss: 1.0129, Accuracy: 0.6506\n",
      "Train-epoch 68. Iteration 00400, Avg-Loss: 1.0099, Accuracy: 0.6499\n",
      "Train-epoch 68. Iteration 00500, Avg-Loss: 1.0124, Accuracy: 0.6477\n",
      "Validation-epoch 68. Avg-Loss: 1.3759, Accuracy: 52.2600\n",
      "--------------------------------------------------\n",
      "Train-epoch 69. Iteration 00100, Avg-Loss: 1.0007, Accuracy: 0.6603\n",
      "Train-epoch 69. Iteration 00200, Avg-Loss: 0.9996, Accuracy: 0.6553\n",
      "Train-epoch 69. Iteration 00300, Avg-Loss: 1.0097, Accuracy: 0.6503\n",
      "Train-epoch 69. Iteration 00400, Avg-Loss: 1.0055, Accuracy: 0.6508\n",
      "Train-epoch 69. Iteration 00500, Avg-Loss: 1.0071, Accuracy: 0.6499\n",
      "Validation-epoch 69. Avg-Loss: 1.3164, Accuracy: 53.6000\n",
      "--------------------------------------------------\n",
      "Train-epoch 70. Iteration 00100, Avg-Loss: 0.9762, Accuracy: 0.6620\n",
      "Train-epoch 70. Iteration 00200, Avg-Loss: 0.9949, Accuracy: 0.6555\n",
      "Train-epoch 70. Iteration 00300, Avg-Loss: 0.9964, Accuracy: 0.6540\n",
      "Train-epoch 70. Iteration 00400, Avg-Loss: 1.0003, Accuracy: 0.6525\n",
      "Train-epoch 70. Iteration 00500, Avg-Loss: 1.0007, Accuracy: 0.6531\n",
      "Validation-epoch 70. Avg-Loss: 1.2946, Accuracy: 54.7700\n",
      "--------------------------------------------------\n",
      "Train-epoch 71. Iteration 00100, Avg-Loss: 0.9899, Accuracy: 0.6657\n",
      "Train-epoch 71. Iteration 00200, Avg-Loss: 0.9914, Accuracy: 0.6605\n",
      "Train-epoch 71. Iteration 00300, Avg-Loss: 0.9999, Accuracy: 0.6562\n",
      "Train-epoch 71. Iteration 00400, Avg-Loss: 0.9998, Accuracy: 0.6541\n",
      "Train-epoch 71. Iteration 00500, Avg-Loss: 0.9980, Accuracy: 0.6543\n",
      "Validation-epoch 71. Avg-Loss: 1.3654, Accuracy: 53.0600\n",
      "--------------------------------------------------\n",
      "Train-epoch 72. Iteration 00100, Avg-Loss: 0.9762, Accuracy: 0.6671\n",
      "Train-epoch 72. Iteration 00200, Avg-Loss: 0.9793, Accuracy: 0.6607\n",
      "Train-epoch 72. Iteration 00300, Avg-Loss: 0.9848, Accuracy: 0.6600\n",
      "Train-epoch 72. Iteration 00400, Avg-Loss: 0.9876, Accuracy: 0.6594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 72. Iteration 00500, Avg-Loss: 0.9874, Accuracy: 0.6589\n",
      "Validation-epoch 72. Avg-Loss: 1.3142, Accuracy: 54.0100\n",
      "--------------------------------------------------\n",
      "Train-epoch 73. Iteration 00100, Avg-Loss: 0.9869, Accuracy: 0.6675\n",
      "Train-epoch 73. Iteration 00200, Avg-Loss: 0.9750, Accuracy: 0.6665\n",
      "Train-epoch 73. Iteration 00300, Avg-Loss: 0.9763, Accuracy: 0.6637\n",
      "Train-epoch 73. Iteration 00400, Avg-Loss: 0.9812, Accuracy: 0.6613\n",
      "Train-epoch 73. Iteration 00500, Avg-Loss: 0.9794, Accuracy: 0.6613\n",
      "Validation-epoch 73. Avg-Loss: 1.2834, Accuracy: 55.6200\n",
      "--------------------------------------------------\n",
      "Train-epoch 74. Iteration 00100, Avg-Loss: 0.9624, Accuracy: 0.6749\n",
      "Train-epoch 74. Iteration 00200, Avg-Loss: 0.9667, Accuracy: 0.6687\n",
      "Train-epoch 74. Iteration 00300, Avg-Loss: 0.9678, Accuracy: 0.6670\n",
      "Train-epoch 74. Iteration 00400, Avg-Loss: 0.9698, Accuracy: 0.6655\n",
      "Train-epoch 74. Iteration 00500, Avg-Loss: 0.9737, Accuracy: 0.6622\n",
      "Validation-epoch 74. Avg-Loss: 1.3645, Accuracy: 52.7900\n",
      "--------------------------------------------------\n",
      "Train-epoch 75. Iteration 00100, Avg-Loss: 0.9548, Accuracy: 0.6695\n",
      "Train-epoch 75. Iteration 00200, Avg-Loss: 0.9632, Accuracy: 0.6706\n",
      "Train-epoch 75. Iteration 00300, Avg-Loss: 0.9652, Accuracy: 0.6680\n",
      "Train-epoch 75. Iteration 00400, Avg-Loss: 0.9684, Accuracy: 0.6659\n",
      "Train-epoch 75. Iteration 00500, Avg-Loss: 0.9688, Accuracy: 0.6652\n",
      "Validation-epoch 75. Avg-Loss: 1.3230, Accuracy: 54.0000\n",
      "--------------------------------------------------\n",
      "Train-epoch 76. Iteration 00100, Avg-Loss: 0.9552, Accuracy: 0.6823\n",
      "Train-epoch 76. Iteration 00200, Avg-Loss: 0.9548, Accuracy: 0.6773\n",
      "Train-epoch 76. Iteration 00300, Avg-Loss: 0.9577, Accuracy: 0.6719\n",
      "Train-epoch 76. Iteration 00400, Avg-Loss: 0.9611, Accuracy: 0.6698\n",
      "Train-epoch 76. Iteration 00500, Avg-Loss: 0.9610, Accuracy: 0.6685\n",
      "Validation-epoch 76. Avg-Loss: 1.3320, Accuracy: 54.2100\n",
      "--------------------------------------------------\n",
      "Train-epoch 77. Iteration 00100, Avg-Loss: 0.9615, Accuracy: 0.6675\n",
      "Train-epoch 77. Iteration 00200, Avg-Loss: 0.9518, Accuracy: 0.6725\n",
      "Train-epoch 77. Iteration 00300, Avg-Loss: 0.9531, Accuracy: 0.6709\n",
      "Train-epoch 77. Iteration 00400, Avg-Loss: 0.9557, Accuracy: 0.6691\n",
      "Train-epoch 77. Iteration 00500, Avg-Loss: 0.9592, Accuracy: 0.6687\n",
      "Validation-epoch 77. Avg-Loss: 1.2951, Accuracy: 54.9200\n",
      "--------------------------------------------------\n",
      "Train-epoch 78. Iteration 00100, Avg-Loss: 0.9375, Accuracy: 0.6830\n",
      "Train-epoch 78. Iteration 00200, Avg-Loss: 0.9496, Accuracy: 0.6737\n",
      "Train-epoch 78. Iteration 00300, Avg-Loss: 0.9516, Accuracy: 0.6725\n",
      "Train-epoch 78. Iteration 00400, Avg-Loss: 0.9551, Accuracy: 0.6702\n",
      "Train-epoch 78. Iteration 00500, Avg-Loss: 0.9530, Accuracy: 0.6707\n",
      "Validation-epoch 78. Avg-Loss: 1.3336, Accuracy: 53.6800\n",
      "--------------------------------------------------\n",
      "Train-epoch 79. Iteration 00100, Avg-Loss: 0.9490, Accuracy: 0.6774\n",
      "Train-epoch 79. Iteration 00200, Avg-Loss: 0.9459, Accuracy: 0.6742\n",
      "Train-epoch 79. Iteration 00300, Avg-Loss: 0.9471, Accuracy: 0.6716\n",
      "Train-epoch 79. Iteration 00400, Avg-Loss: 0.9467, Accuracy: 0.6731\n",
      "Train-epoch 79. Iteration 00500, Avg-Loss: 0.9466, Accuracy: 0.6738\n",
      "Validation-epoch 79. Avg-Loss: 1.4621, Accuracy: 50.3700\n",
      "--------------------------------------------------\n",
      "Train-epoch 80. Iteration 00100, Avg-Loss: 0.9282, Accuracy: 0.6920\n",
      "Train-epoch 80. Iteration 00200, Avg-Loss: 0.9366, Accuracy: 0.6818\n",
      "Train-epoch 80. Iteration 00300, Avg-Loss: 0.9413, Accuracy: 0.6778\n",
      "Train-epoch 80. Iteration 00400, Avg-Loss: 0.9400, Accuracy: 0.6764\n",
      "Train-epoch 80. Iteration 00500, Avg-Loss: 0.9392, Accuracy: 0.6770\n",
      "Validation-epoch 80. Avg-Loss: 1.3287, Accuracy: 54.0200\n",
      "--------------------------------------------------\n",
      "Train-epoch 81. Iteration 00100, Avg-Loss: 0.9170, Accuracy: 0.6944\n",
      "Train-epoch 81. Iteration 00200, Avg-Loss: 0.9333, Accuracy: 0.6817\n",
      "Train-epoch 81. Iteration 00300, Avg-Loss: 0.9345, Accuracy: 0.6802\n",
      "Train-epoch 81. Iteration 00400, Avg-Loss: 0.9342, Accuracy: 0.6793\n",
      "Train-epoch 81. Iteration 00500, Avg-Loss: 0.9363, Accuracy: 0.6782\n",
      "Validation-epoch 81. Avg-Loss: 1.2916, Accuracy: 55.2700\n",
      "--------------------------------------------------\n",
      "Train-epoch 82. Iteration 00100, Avg-Loss: 0.9245, Accuracy: 0.6886\n",
      "Train-epoch 82. Iteration 00200, Avg-Loss: 0.9268, Accuracy: 0.6831\n",
      "Train-epoch 82. Iteration 00300, Avg-Loss: 0.9324, Accuracy: 0.6787\n",
      "Train-epoch 82. Iteration 00400, Avg-Loss: 0.9318, Accuracy: 0.6783\n",
      "Train-epoch 82. Iteration 00500, Avg-Loss: 0.9312, Accuracy: 0.6783\n",
      "Validation-epoch 82. Avg-Loss: 1.3064, Accuracy: 54.7000\n",
      "--------------------------------------------------\n",
      "Train-epoch 83. Iteration 00100, Avg-Loss: 0.9151, Accuracy: 0.6928\n",
      "Train-epoch 83. Iteration 00200, Avg-Loss: 0.9078, Accuracy: 0.6897\n",
      "Train-epoch 83. Iteration 00300, Avg-Loss: 0.9156, Accuracy: 0.6850\n",
      "Train-epoch 83. Iteration 00400, Avg-Loss: 0.9203, Accuracy: 0.6811\n",
      "Train-epoch 83. Iteration 00500, Avg-Loss: 0.9235, Accuracy: 0.6799\n",
      "Validation-epoch 83. Avg-Loss: 1.3515, Accuracy: 53.3300\n",
      "--------------------------------------------------\n",
      "Train-epoch 84. Iteration 00100, Avg-Loss: 0.9164, Accuracy: 0.6914\n",
      "Train-epoch 84. Iteration 00200, Avg-Loss: 0.9094, Accuracy: 0.6900\n",
      "Train-epoch 84. Iteration 00300, Avg-Loss: 0.9157, Accuracy: 0.6858\n",
      "Train-epoch 84. Iteration 00400, Avg-Loss: 0.9164, Accuracy: 0.6848\n",
      "Train-epoch 84. Iteration 00500, Avg-Loss: 0.9182, Accuracy: 0.6845\n",
      "Validation-epoch 84. Avg-Loss: 1.3022, Accuracy: 55.0500\n",
      "--------------------------------------------------\n",
      "Train-epoch 85. Iteration 00100, Avg-Loss: 0.9225, Accuracy: 0.6901\n",
      "Train-epoch 85. Iteration 00200, Avg-Loss: 0.9164, Accuracy: 0.6888\n",
      "Train-epoch 85. Iteration 00300, Avg-Loss: 0.9185, Accuracy: 0.6823\n",
      "Train-epoch 85. Iteration 00400, Avg-Loss: 0.9158, Accuracy: 0.6833\n",
      "Train-epoch 85. Iteration 00500, Avg-Loss: 0.9142, Accuracy: 0.6839\n",
      "Validation-epoch 85. Avg-Loss: 1.3387, Accuracy: 53.7300\n",
      "--------------------------------------------------\n",
      "Train-epoch 86. Iteration 00100, Avg-Loss: 0.9057, Accuracy: 0.6931\n",
      "Train-epoch 86. Iteration 00200, Avg-Loss: 0.9042, Accuracy: 0.6907\n",
      "Train-epoch 86. Iteration 00300, Avg-Loss: 0.9018, Accuracy: 0.6905\n",
      "Train-epoch 86. Iteration 00400, Avg-Loss: 0.9046, Accuracy: 0.6889\n",
      "Train-epoch 86. Iteration 00500, Avg-Loss: 0.9053, Accuracy: 0.6885\n",
      "Validation-epoch 86. Avg-Loss: 1.3546, Accuracy: 53.8500\n",
      "--------------------------------------------------\n",
      "Train-epoch 87. Iteration 00100, Avg-Loss: 0.8899, Accuracy: 0.6978\n",
      "Train-epoch 87. Iteration 00200, Avg-Loss: 0.8939, Accuracy: 0.6950\n",
      "Train-epoch 87. Iteration 00300, Avg-Loss: 0.8954, Accuracy: 0.6935\n",
      "Train-epoch 87. Iteration 00400, Avg-Loss: 0.8986, Accuracy: 0.6920\n",
      "Train-epoch 87. Iteration 00500, Avg-Loss: 0.8981, Accuracy: 0.6913\n",
      "Validation-epoch 87. Avg-Loss: 1.3926, Accuracy: 52.7900\n",
      "--------------------------------------------------\n",
      "Train-epoch 88. Iteration 00100, Avg-Loss: 0.9019, Accuracy: 0.6945\n",
      "Train-epoch 88. Iteration 00200, Avg-Loss: 0.8995, Accuracy: 0.6920\n",
      "Train-epoch 88. Iteration 00300, Avg-Loss: 0.8943, Accuracy: 0.6942\n",
      "Train-epoch 88. Iteration 00400, Avg-Loss: 0.8967, Accuracy: 0.6914\n",
      "Train-epoch 88. Iteration 00500, Avg-Loss: 0.8949, Accuracy: 0.6913\n",
      "Validation-epoch 88. Avg-Loss: 1.3527, Accuracy: 53.7800\n",
      "--------------------------------------------------\n",
      "Train-epoch 89. Iteration 00100, Avg-Loss: 0.9009, Accuracy: 0.7001\n",
      "Train-epoch 89. Iteration 00200, Avg-Loss: 0.8905, Accuracy: 0.6980\n",
      "Train-epoch 89. Iteration 00300, Avg-Loss: 0.8938, Accuracy: 0.6952\n",
      "Train-epoch 89. Iteration 00400, Avg-Loss: 0.8957, Accuracy: 0.6920\n",
      "Train-epoch 89. Iteration 00500, Avg-Loss: 0.8953, Accuracy: 0.6915\n",
      "Validation-epoch 89. Avg-Loss: 1.3192, Accuracy: 54.5800\n",
      "--------------------------------------------------\n",
      "Train-epoch 90. Iteration 00100, Avg-Loss: 0.8796, Accuracy: 0.7018\n",
      "Train-epoch 90. Iteration 00200, Avg-Loss: 0.8817, Accuracy: 0.6988\n",
      "Train-epoch 90. Iteration 00300, Avg-Loss: 0.8846, Accuracy: 0.6966\n",
      "Train-epoch 90. Iteration 00400, Avg-Loss: 0.8835, Accuracy: 0.6959\n",
      "Train-epoch 90. Iteration 00500, Avg-Loss: 0.8831, Accuracy: 0.6956\n",
      "Validation-epoch 90. Avg-Loss: 1.3818, Accuracy: 52.1300\n",
      "--------------------------------------------------\n",
      "Train-epoch 91. Iteration 00100, Avg-Loss: 0.8556, Accuracy: 0.7137\n",
      "Train-epoch 91. Iteration 00200, Avg-Loss: 0.8693, Accuracy: 0.7015\n",
      "Train-epoch 91. Iteration 00300, Avg-Loss: 0.8728, Accuracy: 0.7002\n",
      "Train-epoch 91. Iteration 00400, Avg-Loss: 0.8739, Accuracy: 0.7004\n",
      "Train-epoch 91. Iteration 00500, Avg-Loss: 0.8757, Accuracy: 0.6996\n",
      "Validation-epoch 91. Avg-Loss: 1.3595, Accuracy: 54.0000\n",
      "--------------------------------------------------\n",
      "Train-epoch 92. Iteration 00100, Avg-Loss: 0.8575, Accuracy: 0.7178\n",
      "Train-epoch 92. Iteration 00200, Avg-Loss: 0.8700, Accuracy: 0.7034\n",
      "Train-epoch 92. Iteration 00300, Avg-Loss: 0.8701, Accuracy: 0.7029\n",
      "Train-epoch 92. Iteration 00400, Avg-Loss: 0.8702, Accuracy: 0.7020\n",
      "Train-epoch 92. Iteration 00500, Avg-Loss: 0.8714, Accuracy: 0.7009\n",
      "Validation-epoch 92. Avg-Loss: 1.4038, Accuracy: 53.1500\n",
      "--------------------------------------------------\n",
      "Train-epoch 93. Iteration 00100, Avg-Loss: 0.8431, Accuracy: 0.7116\n",
      "Train-epoch 93. Iteration 00200, Avg-Loss: 0.8526, Accuracy: 0.7070\n",
      "Train-epoch 93. Iteration 00300, Avg-Loss: 0.8614, Accuracy: 0.7017\n",
      "Train-epoch 93. Iteration 00400, Avg-Loss: 0.8652, Accuracy: 0.7007\n",
      "Train-epoch 93. Iteration 00500, Avg-Loss: 0.8697, Accuracy: 0.6997\n",
      "Validation-epoch 93. Avg-Loss: 1.4324, Accuracy: 52.3100\n",
      "--------------------------------------------------\n",
      "Train-epoch 94. Iteration 00100, Avg-Loss: 0.8552, Accuracy: 0.7143\n",
      "Train-epoch 94. Iteration 00200, Avg-Loss: 0.8457, Accuracy: 0.7138\n",
      "Train-epoch 94. Iteration 00300, Avg-Loss: 0.8492, Accuracy: 0.7103\n",
      "Train-epoch 94. Iteration 00400, Avg-Loss: 0.8533, Accuracy: 0.7079\n",
      "Train-epoch 94. Iteration 00500, Avg-Loss: 0.8586, Accuracy: 0.7046\n",
      "Validation-epoch 94. Avg-Loss: 1.3553, Accuracy: 54.2800\n",
      "--------------------------------------------------\n",
      "Train-epoch 95. Iteration 00100, Avg-Loss: 0.8391, Accuracy: 0.7202\n",
      "Train-epoch 95. Iteration 00200, Avg-Loss: 0.8488, Accuracy: 0.7107\n",
      "Train-epoch 95. Iteration 00300, Avg-Loss: 0.8523, Accuracy: 0.7067\n",
      "Train-epoch 95. Iteration 00400, Avg-Loss: 0.8567, Accuracy: 0.7050\n",
      "Train-epoch 95. Iteration 00500, Avg-Loss: 0.8589, Accuracy: 0.7059\n",
      "Validation-epoch 95. Avg-Loss: 1.3458, Accuracy: 54.2900\n",
      "--------------------------------------------------\n",
      "Train-epoch 96. Iteration 00100, Avg-Loss: 0.8410, Accuracy: 0.7158\n",
      "Train-epoch 96. Iteration 00200, Avg-Loss: 0.8443, Accuracy: 0.7107\n",
      "Train-epoch 96. Iteration 00300, Avg-Loss: 0.8469, Accuracy: 0.7106\n",
      "Train-epoch 96. Iteration 00400, Avg-Loss: 0.8481, Accuracy: 0.7098\n",
      "Train-epoch 96. Iteration 00500, Avg-Loss: 0.8503, Accuracy: 0.7081\n",
      "Validation-epoch 96. Avg-Loss: 1.3882, Accuracy: 53.4800\n",
      "--------------------------------------------------\n",
      "Train-epoch 97. Iteration 00100, Avg-Loss: 0.8497, Accuracy: 0.7169\n",
      "Train-epoch 97. Iteration 00200, Avg-Loss: 0.8373, Accuracy: 0.7193\n",
      "Train-epoch 97. Iteration 00300, Avg-Loss: 0.8441, Accuracy: 0.7148\n",
      "Train-epoch 97. Iteration 00400, Avg-Loss: 0.8501, Accuracy: 0.7108\n",
      "Train-epoch 97. Iteration 00500, Avg-Loss: 0.8488, Accuracy: 0.7101\n",
      "Validation-epoch 97. Avg-Loss: 1.3088, Accuracy: 55.2700\n",
      "--------------------------------------------------\n",
      "Train-epoch 98. Iteration 00100, Avg-Loss: 0.8418, Accuracy: 0.7232\n",
      "Train-epoch 98. Iteration 00200, Avg-Loss: 0.8286, Accuracy: 0.7239\n",
      "Train-epoch 98. Iteration 00300, Avg-Loss: 0.8287, Accuracy: 0.7227\n",
      "Train-epoch 98. Iteration 00400, Avg-Loss: 0.8378, Accuracy: 0.7166\n",
      "Train-epoch 98. Iteration 00500, Avg-Loss: 0.8418, Accuracy: 0.7150\n",
      "Validation-epoch 98. Avg-Loss: 1.3291, Accuracy: 54.3400\n",
      "--------------------------------------------------\n",
      "Train-epoch 99. Iteration 00100, Avg-Loss: 0.8315, Accuracy: 0.7174\n",
      "Train-epoch 99. Iteration 00200, Avg-Loss: 0.8332, Accuracy: 0.7143\n",
      "Train-epoch 99. Iteration 00300, Avg-Loss: 0.8329, Accuracy: 0.7135\n",
      "Train-epoch 99. Iteration 00400, Avg-Loss: 0.8355, Accuracy: 0.7116\n",
      "Train-epoch 99. Iteration 00500, Avg-Loss: 0.8334, Accuracy: 0.7122\n",
      "Validation-epoch 99. Avg-Loss: 1.4535, Accuracy: 51.7100\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Removes, the need to call F.to_image ourselves.\n",
    "# Also, please look up what transforms.Normalize does.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training, and validation datasets.\n",
    "trainset = CIFAR10(root='./data',\n",
    "                   train=True,\n",
    "                   transform=transform,\n",
    "                   download=True)\n",
    "valset = CIFAR10(root='./data',\n",
    "                 train=False,\n",
    "                 transform=transform,\n",
    "                 download=True)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Shuffling is needed in case dataset is not shuffled by default.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                           batch_size=batchSize,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# We don't need to bach the validation set but let's do it anyway.\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,\n",
    "                                         batch_size=batchSize,\n",
    "                                         shuffle=False)  # No need.\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define a learning rate.\n",
    "learningRate = 1e-3\n",
    "\n",
    "# Define number of epochs.\n",
    "N = 100\n",
    "\n",
    "# Create the model.\n",
    "n_inputs = 3 * 32 * 32\n",
    "nHidden = 524  #Neurons\n",
    "nOutputs = 10  #10 classes on fashionmnist\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model = {}\n",
    "model['linear1'] = toynn_Linear(n_inputs, nHidden)\n",
    "model['relu'] = toynn_ReLU()\n",
    "model['linear2'] = toynn_Linear(nHidden, nHidden)\n",
    "model['relu1'] = toynn_ReLU()\n",
    "model['linear3'] = toynn_Linear(nHidden, nOutputs)\n",
    "model['loss'] = toynn_CrossEntropyLoss()\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# log accuracies and losses.\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "saved_models = {}\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# param_grid = {\"learning_rate\" : [.0001, .001], \"nHidden\" : [256,512,1024]}\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(0, N):\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    # Make a pass over the training data.\n",
    "    for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "        # .view appears to be similar to np.reshape. We're just flattening and reshaping inputs into a new tensor shape here.\n",
    "        inputs = inputs.view(\n",
    "            batchSize, n_inputs)  #Returns Torch Tensor batchSize X Image Size Flattened\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "        #Forward\n",
    "        a = model['linear1'].forward(inputs)  # torch.Size([100, 512])\n",
    "        z = model['relu'].forward(a)  # torch.Size([100, 512])\n",
    "        a1 = model['linear2'].forward(\n",
    "            z\n",
    "        )  # torch.Size([100, 10]) #This is computed as softmax in the loss for CrossEntropy\n",
    "        z1 = model['relu1'].forward(a1)\n",
    "        y_hat = model['linear3'].forward(z1)\n",
    "        cum_loss += model['loss'].forward(y_hat, labels).item()\n",
    "\n",
    "        # Count how many correct in this batch.\n",
    "        max_scores, max_labels = y_hat.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        #Backward pass. (Gradient computation stage)\n",
    "        y_hat_grads = model['loss'].backward(y_hat,\n",
    "                                             labels)  #torch.Size([10, 10])\n",
    "        z1_grads = model['linear3'].backward(z1, y_hat_grads)\n",
    "        a1_grads = model['relu'].backward(a1, z1_grads)\n",
    "        z_grads = model['linear2'].backward(z, a1_grads)\n",
    "        a_grads = model['relu'].backward(a, z_grads)\n",
    "        x_grads = model['linear1'].backward(inputs, a_grads)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Parameter updates (SGD step).\n",
    "#         decay = 10.\n",
    "#         learningRate = (1. / (1. + decay * (epoch+1)))\n",
    "    \n",
    "    \n",
    "        model['linear1'].weight.add_(-learningRate,\n",
    "                                     model['linear1'].weight_grads)\n",
    "        model['linear1'].bias.add_(-learningRate, model['linear1'].bias_grads)\n",
    "        model['linear2'].weight.add_(-learningRate,\n",
    "                                     model['linear2'].weight_grads)\n",
    "        model['linear2'].bias.add_(-learningRate, model['linear2'].bias_grads)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Logging the current results on training.\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i * batchSize + 1)))\n",
    "    \n",
    "    train_accuracies.append(correct / len(trainset))\n",
    "    train_losses.append(cum_loss / (i + 1))\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Make a pass over the validation data.\n",
    "    correct = 0.0\n",
    "    cum_loss = 0.0\n",
    "    for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "        inputs = inputs.view(batchSize, n_inputs)\n",
    "\n",
    "        #Forward\n",
    "        a = model['linear1'].forward(inputs)  # torch.Size([100, 512])\n",
    "        z = model['relu'].forward(a)  # torch.Size([100, 512])\n",
    "        a1 = model['linear2'].forward(\n",
    "            z\n",
    "        )  # torch.Size([100, 10]) #This is computed as softmax in the loss for CrossEntropy\n",
    "        z1 = model['relu1'].forward(a1)\n",
    "        y_hat = model['linear3'].forward(z1)\n",
    "\n",
    "        cum_loss += model['loss'].forward(y_hat, labels).item()\n",
    "\n",
    "        # Count how many correct in this batch.\n",
    "        max_scores, max_labels = y_hat.max(1)\n",
    "        correct += (max_labels == labels).sum().item()\n",
    "\n",
    "    val_accuracies.append(correct / len(valset))\n",
    "    val_losses.append(cum_loss / (i + 1))\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #Early Stopping\n",
    "    patience = 10\n",
    "#     if epoch > 5:\n",
    "#         if not any(i >= val_losses[-1] for i in val_losses[-patience:-1]):\n",
    "#             print('Early Stopping! Val Loss has not decreased in last %s batch iterations' % patience)\n",
    "#             break\n",
    "\n",
    "    # Logging the current results on validation.\n",
    "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "          (epoch, cum_loss / (i + 1), correct / inputs.shape[0]))\n",
    "    saved_models[epoch] = model #Checkpointing\n",
    "    print('-' *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:16:53.567475Z",
     "start_time": "2020-02-06T20:16:53.344047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max validation accuracy: 55.62 ; Epochs: 73 \n"
     ]
    }
   ],
   "source": [
    "# Your code for visualizing train/val plots for accuracy and losses.\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_losses, 'bo-', label='val-loss')\n",
    "plt.plot(train_losses, 'ro-', label='train-loss')\n",
    "plt.grid('on')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['validation', 'training'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, 'bo-', label='val-acc')\n",
    "plt.plot(train_accuracies, 'ro-', label='train-acc')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid('on')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['validation', 'training'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print('max validation accuracy: %.2f ; Epochs: %.0f ' % (max(val_accuracies) * 100, np.argmax(val_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:52:59.845977Z",
     "start_time": "2020-02-06T20:52:59.841988Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = saved_models[np.argmax(val_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T23:56:16.934036Z",
     "start_time": "2020-02-06T23:56:16.733592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 50 is a truck\n",
      "Model Predicts truck with 96.61 percent confidence\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 55 is a ship\n",
      "Model Predicts ship with 50.42 percent confidence\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 60 is a horse\n",
      "Model Predicts horse with 63.86 percent confidence\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 65 is a bird\n",
      "Model Predicts bird with 87.01 percent confidence\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 70 is a bird\n",
      "Model Predicts deer with 37.86 percent confidence\n"
     ]
    }
   ],
   "source": [
    "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "image_index = [50,55,60,65,70]\n",
    "\n",
    "for i in image_index:\n",
    "    img, label = val_loader.dataset[i]\n",
    "    inputs = img.view(1, n_inputs)\n",
    "\n",
    "    a = best_model['linear1'].forward(inputs)  # torch.Size([100, 512])\n",
    "    z = best_model['relu'].forward(a)  # torch.Size([100, 512])\n",
    "    a1 = best_model['linear2'].forward(\n",
    "        z\n",
    "    )  # torch.Size([100, 10]) #This is computed as softmax in the loss for CrossEntropy\n",
    "    z1 = best_model['relu1'].forward(a1)\n",
    "    y_hat = best_model['linear3'].forward(z1)\n",
    "\n",
    "    max_scores, max_labels = y_hat.softmax(dim = 1).max(1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(img.transpose(0,2).transpose(0,1));\n",
    "    plt.grid(False); plt.axis('off')\n",
    "    plt.show()\n",
    "    print('Image {0} is a {1}'.format(i, classes[label]))\n",
    "    print('Model Predicts %s with %.2f percent confidence' % (classes[max_labels], max_scores[0] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VLqNpM79-3l"
   },
   "source": [
    "**I obtained a validation accuracy of 55.62% after 73 epochs. Best model was checkpointed and used for inference.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Y5AUSc3jufKi",
    "PGdZ-bmxb473",
    "ajJ1z9qb0oqt",
    "tAMnCGwv-zJN",
    "LGIxg3q5i_bl",
    "gXlKZ2eQj9Y7"
   ],
   "name": "Copy of deep_learning_assignment_spring2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
